{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIRST_NAME = \"Jacob\"\n",
    "LAST_NAME = \"Friedman\"\n",
    "STUDENT_ID = \"801444589\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For using google collab when repo has been uploaded to your google drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# %cd /content/drive/MyDrive/CourseProject_JFriedman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get started locally quickly, create a Python 3.11 environment with conda using the following command:\n",
    "# conda create -n fastai python=3.11\n",
    "# conda activate fastai\n",
    "\n",
    "# Setup torch for GPU with CUDA 12.4 & fastai - this will work for local setups as well as long as CUDA toolkit is installed\n",
    "# If working locally this requires CUDA Toolbox 12.4 installed to your computer with a compatible GPU: https://developer.nvidia.com/cuda-gpus\n",
    "# Ensure CUDA Toobox 12.4 is downloaded and installed: https://developer.nvidia.com/cuda-12-4-0-download-archive\n",
    "# Must have proper environment variables set up for PATH, refer to NVIDIA documentation\n",
    "# Torch 2.5.1 is the latest version supported by fastai and CUDA 12.4 is the latest version supported by Torch 2.5.1\n",
    "# Also installs seaborn for data visualization and torchsummary for model summaries\n",
    "%pip install -Uqq seaborn --upgrade-strategy only-if-needed\n",
    "%pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu124 --upgrade-strategy only-if-needed\n",
    "%pip install torchsummary --upgrade-strategy only-if-needed\n",
    "%pip install fastai --upgrade-strategy only-if-needed\n",
    "%pip install nltk --upgrade-strategy only-if-needed\n",
    "%pip install scikit-optimize --upgrade-strategy only-if-needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Goal \n",
    "#### The goal of this project is to reimplement the agent used in the paper **\"A Context-Aware Approach for Detecting Worth-Checking Claims in Political Debates\"** but instead of using the original implementation we are going to try an RNN implementation and a Transformer implementation. We will utilize Grid Search to find the best hyperparameters for our models. We will follow the same pre-processing steps and utilize the same data for comparative accuracy.\n",
    "\n",
    "---\n",
    "#### Some code has been reused, or refactored, as originally developed by the author's of the paper. They have requested the following annotations be included:\n",
    "\n",
    "```bib\n",
    "@InProceedings{RANLP2017:debates,\n",
    "  author    = {Pepa Gencheva and Preslav Nakov and Llu\\'{i}s M\\`{a}rquez and Alberto Barr\\'on-Cede\\~no and Ivan Koychev},\n",
    "  title     = {{A Context-Aware Approach for Detecting Worth-Checking Claims in Political Debates},\n",
    "  booktitle = {Proceedings of the 2017 International Conference on Recent Advances in Natural Language Processing},\n",
    "  month     = {September},\n",
    "  year      = {2017},\n",
    "  address   = {Varna, Bulgaria},\n",
    "  series    = {RANLP~'17}\n",
    "}\n",
    "```\n",
    "---\n",
    "\n",
    "## Author's Original Implementation Architechture\n",
    "#### The **feed-forward neural network** designed as a multitask learning model, as implemented by the authors, is designed to classify check-worthy claims in political debates by leveraging shared and task-specific learning. The architecture begins with a **shared input layer** that processes the input features and learns general representations through a dense layer with 1000 units, LeakyReLU activation, and dropout for regularization. This shared layer captures common patterns across tasks.\n",
    "\n",
    "The model then branches into two **task-specific layers**:\n",
    "1. **General Check-Worthiness Branch (`pred_any`)**: This branch predicts whether a claim is check-worthy in a general sense, independent of specific criteria or datasets. It uses a dense layer with 500 units, LeakyReLU activation, dropout, and a final sigmoid output layer for binary classification.\n",
    "2. **PolitiFact-Specific Branch (`pred_pf`)**: This branch specializes in identifying check-worthy claims based on **PolitiFact's annotations**, which reflect specific fact-checking criteria. It also uses a dense layer with 500 units, LeakyReLU activation, dropout, and a sigmoid output layer.\n",
    "\n",
    "The model is trained using the **Stochastic Gradient Descent (SGD)** optimizer with **Nesterov momentum** (momentum = 0.9) and a learning rate of 0.006. The loss function for both tasks is **binary cross-entropy**, and the model uses **callbacks** such as **early stopping** (to prevent overfitting) and model checkpointing (to save the best weights based on validation accuracy). This multitask setup enables the model to learn shared representations in the shared layer while optimizing for two related but distinct tasks in the task-specific branches. This approach improves the model's ability to generalize across tasks while maintaining specialization for specific datasets or criteria.\n",
    "\n",
    "---\n",
    "## Setup Project Path in config.ini\n",
    "### Before continuing any further you **must** set your **project_path** varaiable to the proper directory on your machine where you have downloaded this project. The file is located on the top level of the project directory.\n",
    "### **Failure to do this will result in the project failing to run**\n",
    "---\n",
    "## Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "\n",
    "# Set the device to be used for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import fastai\n",
    "print(f\"FastAI version: {fastai.__version__}\")\n",
    "\n",
    "# Set for CUDA to ensure that memory is properly allocated\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Data Pre-Processing**\n",
    "### This data pre-processing step has been slightly refactored from the original code as developed by the authors. Given the complex nature of their feature set, I decided to use their code to prevent having to reimplment features I may not fully understand. And to prevent and possible issues in evaluation of my results to their results. Some refactoring was done for modernization.\n",
    "---\n",
    "### **1.1 Ensure punkt and punkt_tab are Installed**\n",
    "##### These are necessary required packages needed to use the author's pre-processing packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure punkt and punkt_tab are downloaded\n",
    "# They are necessary for author's code to run\n",
    "import nltk\n",
    "from src.utils.config import get_config\n",
    "\n",
    "config = get_config()\n",
    "\n",
    "nltk.download('punkt', download_dir=config['data'])\n",
    "nltk.download('punkt_tab', download_dir=config['data'])\n",
    "\n",
    "# Add the directory where 'punkt' is located to NLTK's search paths\n",
    "nltk.data.path.append(config['data'])\n",
    "\n",
    "print(f\"NLTK version: {nltk.__version__}\")\n",
    "print(\"NLTK data paths:\", nltk.data.path)\n",
    "\n",
    "# Verify that the 'punkt' and 'punkt_tab' resource is available\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    print(\"punkt tokenizer is available.\")\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "    print(\"punkt_tab tokenizer is available.\")\n",
    "except LookupError:\n",
    "    print(\"punkt tokenizer is NOT available. Attempting to download...\")\n",
    "    nltk.download('punkt', download_dir=config['data'])\n",
    "    print(\"punkt_tab tokenizer is NOT available. Attempting to download...\")\n",
    "    nltk.download('punkt_tab', download_dir=config['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **1.2 Continue With Pre-Processing Data**\n",
    "\n",
    "#### **How the Authors Did Their Pre-Processing**\n",
    "#### In summary, the data preprocessing involves loading the debate text, converting it into numerical features, and extracting the corresponding labels. The core of this process is the feature extraction pipeline, which transforms the raw text into a numerical representation that the machine learning model can use.\n",
    "\n",
    "1.  **Loading the Debate Data:**\n",
    "\n",
    "    * They use the `read_debates()` function (defined in `src/data/debates.py`) to load the debate data. This function reads the text from different debates and organizes it.\n",
    "    * The data is divided into two sets: a training set and a validation set. The training set is used to train the model, and the validation set is used to evaluate its performance.\n",
    "    * Essentially, this step involves gathering the raw text data and splitting it into training and validation sets.\n",
    "\n",
    "2.  **Feature Extraction:**\n",
    "\n",
    "    * This is a crucial step where the text data is converted into numerical features. Which is necessary for us to be able to use the data for Machine Learning\n",
    "    * They use a method called `get_serialized_pipeline()` (defined in `src/features/feature_sets.py`) to extract these features using a custom pipeline. This pipeline includes several components that extract different types of information from the text:\n",
    "        * **Contextual features:** These features capture information about the surrounding text of a given claim.\n",
    "        * **POS Tags:** Part-of-speech tags (e.g., noun, verb, adjective) are extracted to provide syntactic information.\n",
    "        * **Stylistic features:** These features capture the writing style of the text.\n",
    "    * The pipeline uses pre-computed features (serialized) for efficiency reasons. This means that the feature extraction process was done beforehand, and the code just loads the extracted features.\n",
    "    * The `fit_transform()` method is used to extract features from the training data, and the `transform()` method is used to extract features from the validation data.\n",
    "    * In essence, this step transforms the text data into a numerical matrix representation that we need for our model.\n",
    "\n",
    "3.  **Label Extraction:**\n",
    "\n",
    "    * To train the model, the authors need to provide it with the correct answers (labels). In this case, the labels indicate whether a claim is worth fact-checking.\n",
    "    * The code extracts labels for different evaluation scenarios:\n",
    "        * `y_train_any`, `y_val_any`: These are binary labels representing a general assessment of whether a claim is check-worthy from any source.\n",
    "        * `y_train_pf`, `y_val_pf`: These labels are extracted from `s.labels[5]` and correspond to PolitiFact's assessment.\n",
    "        * `y_train_wp`, `y_val_wp`: These labels are extracted from `s.labels[4]` and correspond to the Washington Post's assessment.\n",
    "    * This step involves preparing the target variables for training and evaluating the model's performance against different fact-checking sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.debates import read_debates, Debate\n",
    "from src.features.feature_sets import get_cb_pipeline, get_serialized_pipeline\n",
    "from src.stats.rank_metrics import average_precision, precision_at_n\n",
    "\n",
    "# Step 1: Prepare Training and Validation Data\n",
    "# Read debates for training and validation\n",
    "train_sentences = (\n",
    "    read_debates(Debate.FIRST) +\n",
    "    read_debates(Debate.VP) +\n",
    "    read_debates(Debate.SECOND)\n",
    ")\n",
    "val_sentences = read_debates(Debate.THIRD)\n",
    "\n",
    "# Step 2: Initialize and Fit the Pipeline\n",
    "# Use the serialized pipeline for feature extraction\n",
    "pipeline = get_serialized_pipeline(train=train_sentences)\n",
    "\n",
    "# Transform training and validation data\n",
    "X_train = pipeline.fit_transform(train_sentences)\n",
    "X_val = pipeline.transform(val_sentences)\n",
    "\n",
    "# Step 3: Prepare Data for Various Test Cases\n",
    "# Extract labels for different test cases: any, PolitiFact, and WP\n",
    "y_train_any = np.array([1 if s.label > 0 else 0 for s in train_sentences]) # Binary labels\n",
    "y_train_pf = np.array([s.labels[5] for s in train_sentences]) # PolitiFact labels\n",
    "y_train_wp = np.array([s.labels[4] for s in train_sentences]) # WP labels\n",
    "\n",
    "y_val_any = np.array([1 if s.label > 0 else 0 for s in val_sentences]) # Binary labels\n",
    "y_val_pf = np.array([s.labels[5] for s in val_sentences]) # PolitiFact labels\n",
    "y_val_wp = np.array([s.labels[4] for s in val_sentences]) # WP labels\n",
    "\n",
    "# Step 4: Debugging and Output\n",
    "# Print shapes and sample data for debugging\n",
    "print(f\"Training feature matrix shape: {X_train.shape}\")\n",
    "print(f\"Sample labels for train_sentences[103]: {train_sentences[103].labels}\")\n",
    "print(f\"Binary label for train_sentences[103]: {train_sentences[103].label}\")\n",
    "print(f\"Shape of first training feature vector: {X_train[0].shape}\")\n",
    "print(f\"Shape of y_train_all: {y_train_any.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **2. Model Implementation**\n",
    "\n",
    "---\n",
    "\n",
    "### **2.1.1 RNN Model Definition**\n",
    "\n",
    "#### Let's start by defining an RNN using the same architechture and implementation as the authors original implementation, as described above in the section \"**Author's Original Implementation Architechture**\" utilizing scikit-learn, fastai, and torch.\n",
    "\n",
    "#### **MultiTaskRNN Model**\n",
    "\n",
    "This `MultiTaskRNN` model is a multi-task Recurrent Neural Network (RNN) designed for sequence processing tasks. It employs a shared LSTM layer to capture sequential dependencies in the input data, followed by task-specific output branches.\n",
    "\n",
    "**Architecture:**\n",
    "\n",
    "* **Shared LSTM Layer:**\n",
    "    *     The model uses a shared Long Short-Term Memory (LSTM) layer.\n",
    "    *     This layer processes the input sequence (`input_size`) and generates hidden state representations.\n",
    "    *     Key parameters include:\n",
    "        *     `input_size`: Defines the number of expected features in the input.\n",
    "        *     `hidden_size`: Specifies the number of features in the hidden state.\n",
    "        *     `num_layers`: Indicates the number of recurrent layers.\n",
    "        *     `dropout`: Dropout rate for regularization.\n",
    "        *     `batch_first`: If `True`, the input and output tensors are provided as (batch, sequence, feature).\n",
    "        *     `bidrectional`: If `True`, the input sequence will be analyzed forwards and backwards.\n",
    "\n",
    "* **Task-Specific Branches:**\n",
    "    *     The model includes two task-specific branches (`pred_any`, `pred_pf`).\n",
    "    *     Each branch is a sequential network consisting of:\n",
    "        *     A linear layer with 500 output units.\n",
    "        *     Leaky ReLU activation.\n",
    "        *     Dropout layer for regularization.\n",
    "        *     A linear layer with 1 output unit.\n",
    "        *     Sigmoid activation for binary classification.\n",
    "\n",
    "**Forward Pass:**\n",
    "\n",
    "1.  **Input Processing:**\n",
    "    *     The `forward` method takes an input tensor `x`.\n",
    "    *     It handles cases where `x` might be a tuple (input, targets) by extracting the input.\n",
    "    *     The input is converted to `float32` format.\n",
    "    *     Input dimensions are adjusted to match the LSTM's expected input shape (batch\\_size, sequence\\_length, input\\_size).\n",
    "2.  **Shared LSTM:**\n",
    "    *     The input is passed through the shared LSTM layer.\n",
    "    *     The LSTM outputs the hidden state sequence.\n",
    "3.  **Task-Specific Predictions:**\n",
    "    *     The output of the last time step from the LSTM is used as input for both task-specific branches.\n",
    "    *     Each branch produces a single output, representing the prediction for its respective task.\n",
    "4.  **Output:**\n",
    "    *     The `forward` method returns the predictions from both task-specific branches (`pred_any_scores`, `pred_pf_scores`).\n",
    "\n",
    "**Relation to Author's Architecture:**\n",
    "\n",
    "My model adopts a distinct architecture compared to Gencheva et al. (2019), which relied on Support Vector Machines (SVM) and Feed-Forward Neural Networks (FNN). However, a key similarity lies in the emphasis on contextual modeling. While Gencheva et al. (2019) used SVM and FNN, my model employs a shared LSTM layer to capture relationships within the input sequence, processing the input and then using the output of the LSTM as the input for two task-specific branches (pred_any and pred_pf). These branches, like the multi-task approach in Gencheva et al. (2019), are designed to predict check-worthiness. The use of a shared LSTM, followed by task-specific branches, allows the model to learn shared representations relevant to both tasks, while also enabling the capture of task-specific nuances, reflecting Gencheva et al.'s (2019) focus on modeling the context of claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MultiTaskRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        # Shared RNN layer\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        \n",
    "        # Task-specific branches\n",
    "        self.pred_any = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, 500), # Linear layer with 500 output units\n",
    "            nn.LeakyReLU(), # Leaky ReLU activation\n",
    "            nn.Dropout(dropout), # Dropout layer\n",
    "            nn.Linear(500, 1), # Linear layer with 1 output unit\n",
    "            nn.Sigmoid() # Sigmoid activation\n",
    "        )\n",
    "        \n",
    "        self.pred_pf = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, 500), # Linear layer with 500 output units\n",
    "            nn.LeakyReLU(), # Leaky ReLU activation\n",
    "            nn.Dropout(dropout), # Dropout layer\n",
    "            nn.Linear(500, 1), # Linear layer with 1 output unit\n",
    "            nn.Sigmoid() # Sigmoid activation\n",
    "        )\n",
    "\n",
    "    # Forward pass through the network\n",
    "    def forward(self, x):\n",
    "        # Unpack inputs if x is a tuple (e.g., (inputs, targets))\n",
    "        if isinstance(x, tuple):\n",
    "            x = x[0]  # Extract the inputs tensor\n",
    "\n",
    "        # Ensure input is in float32 format\n",
    "        x = x.float()\n",
    "\n",
    "        # Reshape input if necessary to match LSTM's expected input shape\n",
    "        if x.dim() == 2:  # If input is (batch_size, input_size), add a sequence dimension\n",
    "            # For debugging purposes\n",
    "            # print(\"Reshaping input to account for a sequence dimension...\")\n",
    "            x = x.unsqueeze(1)  # Add a sequence length of 1: (batch_size, 1, input_size)\n",
    "\n",
    "        # Pass input through the shared RNN layer\n",
    "        rnn_scores, _ = self.rnn(x)\n",
    "\n",
    "        # Use the output of the last time step for task-specific branches\n",
    "        shared_scores = rnn_scores[:, -1, :]  # Take the last time step's output\n",
    "\n",
    "        # Task-specific branches\n",
    "        pred_any_scores = self.pred_any(shared_scores)\n",
    "        pred_pf_scores = self.pred_pf(shared_scores)\n",
    "\n",
    "        return pred_any_scores, pred_pf_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **2.1.2 Test the Model**\n",
    "#### Test the model to ensure the code is valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model output\n",
    "model = MultiTaskRNN(input_size=X_train.shape[1], hidden_size=64, num_layers=2, dropout=0.2)\n",
    "model = model.to(device)\n",
    "\n",
    "# Sample input tensor\n",
    "sample_input = torch.randn(1, X_train.shape[1]).to(device)\n",
    "output = model(sample_input)\n",
    "\n",
    "# Print the model output\n",
    "print(f\"Model output: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "### **2.2.1 Transformer Model Definition**\n",
    "\n",
    "#### Let's start by defining a Transformer using the same architechture and implementation as the authors original implementation, as described above in the section \"**Author's Original Implementation Architechture**\" utilizing scikit-learn, fastai, and torch.\n",
    "\n",
    "#### **MultiTaskTransformer**\n",
    "\n",
    "The `MultiTaskTransformer` is a neural network model implemented using the PyTorch library. It is designed for multi-task learning, specifically for predicting check-worthiness of claims in political debates.\n",
    "\n",
    "**Architecture:**\n",
    "\n",
    "* **Shared Transformer Encoder:** The model uses a Transformer Encoder as its base. The Transformer Encoder consists of multiple `TransformerEncoderLayer` layers.\n",
    "    *    `d_model`: Specifies the input size, which must match the model dimension.\n",
    "    *    `nhead`: Defines the number of attention heads.\n",
    "    *    `dim_feedforward`: Sets the hidden size of the feedforward layers within the Transformer Encoder Layer.\n",
    "    *    `dropout`: Applies dropout for regularization.\n",
    "    *    `num_layers`: Determines the number of Transformer Encoder Layers.\n",
    "\n",
    "* **Task-Specific Branches:** The model has two task-specific branches: `pred_any` and `pred_pf`. Both branches are sequential neural networks:\n",
    "    *    They consist of linear layers (`nn.Linear`), Leaky ReLU activation (`nn.LeakyReLU`), Dropout layers (`nn.Dropout`), and a final linear layer to produce a single output.\n",
    "    *    The final layer uses a Sigmoid activation function as these branches are designed for binary classification tasks, in our case, check-worthiness.\n",
    "\n",
    "**Forward Pass:**\n",
    "\n",
    "*    The `forward` function defines how data flows through the network.\n",
    "*    It handles input tensors, ensuring they are in the correct format (float32) and shape for the Transformer. If the input is 2D, it adds a sequence dimension. The input is then transposed to fit the expected input shape of the Transformer.\n",
    "*    The input is passed through the shared Transformer Encoder.\n",
    "*    The output from the last time step of the Transformer Encoder is used as input for the task-specific branches.\n",
    "*    Each task-specific branch (`pred_any`, `pred_pf`) processes this shared output to produce its own prediction.\n",
    "*    The function returns the predictions from both task-specific branches.\n",
    "\n",
    "**Relation to Author's Architecture:**\n",
    "\n",
    "My code uses a Transformer-based architecture, a departure from the Support Vector Machines (SVM) and Feed-Forward Neural Networks (FNN) employed by Gencheva et al. (2019). However, similar to their work, my approach emphasizes contextual modeling. This is achieved through a Transformer Encoder, designed to capture relationships within a sequence of data. The `pred_any` and `pred_pf` task-specific branches likely mirror the paper's multi-task strategy, predicting check-worthiness relative to at least one source, or a specific medium. By using a shared Transformer Encoder followed by task-specific branches, the model learns general representations applicable to all tasks, while also accommodating task-specific nuances. This architecture aligns with the paper's focus on modeling the context of claims, including the relationships between a claim and its surrounding debate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MultiTaskTransformer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_heads, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        # Shared Transformer Encoder\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=input_size, nhead=num_heads, dim_feedforward=hidden_size, dropout=dropout), num_layers=num_layers)\n",
    "\n",
    "        # Task-specific branches\n",
    "        self.pred_any = nn.Sequential(\n",
    "            nn.Linear(input_size, 500),  # Linear layer with 500 output units\n",
    "            nn.LeakyReLU(),  # Leaky ReLU activation\n",
    "            nn.Dropout(dropout),  # Dropout layer\n",
    "            nn.Linear(500, 1),  # Linear layer with 1 output unit\n",
    "            nn.Sigmoid()  # Sigmoid activation\n",
    "        )\n",
    "\n",
    "        self.pred_pf = nn.Sequential(\n",
    "            nn.Linear(input_size, 500),  # Linear layer with 500 output units\n",
    "            nn.LeakyReLU(),  # Leaky ReLU activation\n",
    "            nn.Dropout(dropout),  # Dropout layer\n",
    "            nn.Linear(500, 1),  # Linear layer with 1 output unit\n",
    "            nn.Sigmoid()  # Sigmoid activation\n",
    "        )\n",
    "\n",
    "    # Forward pass through the network\n",
    "    def forward(self, x):\n",
    "        # Unpack inputs if x is a tuple (e.g., (inputs, targets))\n",
    "        if isinstance(x, tuple):\n",
    "            x = x[0]  # Extract the inputs tensor\n",
    "\n",
    "        # Ensure input is in float32 format\n",
    "        x = x.float()\n",
    "\n",
    "        # Reshape input if necessary to match Transformer's expected input shape\n",
    "        if x.dim() == 2:  # If input is (batch_size, input_size), add a sequence dimension\n",
    "            # For Debugging\n",
    "            # print(\"Reshaping input to account for a sequence dimension...\")\n",
    "            x = x.unsqueeze(1)  # Add a sequence length of 1: (batch_size, 1, input_size)\n",
    "\n",
    "        # Transpose input to match Transformer's expected shape: (sequence_length, batch_size, input_size)\n",
    "        x = x.transpose(0, 1)  # Shape becomes (sequence_length, batch_size, input_size)\n",
    "\n",
    "        # Pass input through the shared Transformer Encoder\n",
    "        transformer_output = self.transformer(x)  # Output shape: (sequence_length, batch_size, input_size)\n",
    "\n",
    "        # Use the output of the last time step for task-specific branches\n",
    "        shared_scores = transformer_output[-1, :, :]  # Take the last time step's output (batch_size, input_size)\n",
    "\n",
    "        # Task-specific branches\n",
    "        pred_any_scores = self.pred_any(shared_scores)\n",
    "        pred_pf_scores = self.pred_pf(shared_scores)\n",
    "\n",
    "        return pred_any_scores, pred_pf_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **2.2.2 Test the Model**\n",
    "#### Test the model to ensure the code is valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Transformer model output\n",
    "model = MultiTaskTransformer(input_size=X_train.shape[1], hidden_size=64, num_layers=2, num_heads=4, dropout=0.2)\n",
    "model = model.to(device)\n",
    "\n",
    "# Create a sample input tensor\n",
    "# Transformer expects input in the shape (sequence_length, batch_size, input_size)\n",
    "sequence_length = 10 \n",
    "sample_input = torch.randn(sequence_length, 1, X_train.shape[1]).to(device)  # (sequence_length, batch_size, input_size)\n",
    "\n",
    "# Pass the sample input through the model\n",
    "output = model(sample_input)\n",
    "\n",
    "# Print the model output\n",
    "print(f\"Model output (Task: Any): {output[0]}\")\n",
    "print(f\"Model output (Task: PF): {output[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Helper Classes and Methods**\n",
    "\n",
    "#### The following classes and methods were developed to help the fastai learner deal with the multitask model. Let me walk you through some of the key components of my code:\n",
    "\n",
    "### Core Classes and Functions\n",
    "\n",
    "* **`Callback` (from `fastai.callback.core`)**:\n",
    "    *     This is the base class for callbacks in the fastai library.\n",
    "    *     I'm using it here to create `DebugCallback`, a custom callback that I use for debugging purposes. It simply prints the input, target, predictions, and loss after each batch. This helps me keep an eye on the training process.\n",
    "* **`Metric` (from `fastai.metrics`)**:\n",
    "    *     This is another fastai class that I extend to create custom metrics.\n",
    "    *     I've defined two custom metric classes: `MultiTaskMetric` and `MultiTaskTotalMetric`. These are designed to handle the specific requirements of my multi-task learning setup.\n",
    "* **`OptimWrapper` (from `fastai.optimizer`)**:\n",
    "    *     This class helps in wrapping PyTorch optimizers for use with fastai's training loop.\n",
    "    *     I'm using `partial` from `functools` along with `OptimWrapper` to create optimizer configurations (SGD, Adam, AdamW) with specific learning rates.\n",
    "* **`binary_cross_entropy` (from `torch.nn.functional`)**:\n",
    "    *     This is the standard PyTorch function for calculating binary cross-entropy loss.\n",
    "    *     I'm using this as the core loss function within my custom multi-task loss functions.\n",
    "* **Optimizers (from `torch.optim`)**:\n",
    "    *     I'm using standard PyTorch optimizers: `SGD`, `Adam`, and `AdamW`.\n",
    "    *     These are used to update the model's weights during training.\n",
    "* **`confusion_matrix` (from `sklearn.metrics`)**:\n",
    "    *     This scikit-learn function is used to compute the confusion matrix, which is helpful for evaluating classification performance.\n",
    "    *     I use it within my evaluation function to calculate confusion matrices for each task.\n",
    "\n",
    "### Custom Classes and Functions\n",
    "\n",
    "* **`DebugCallback(Callback)`**:\n",
    "    *     As mentioned earlier, this is a simple callback for printing debug information during training.\n",
    "    *     It's useful for inspecting the data flow and model behavior.\n",
    "* **`MultiTaskMetric(Metric)`**:\n",
    "    *     This custom metric calculates accuracy and ranking metrics (precision, MAP, R-Precision, F1-score) for individual tasks in my multi-task setup.\n",
    "    *     It accumulates predictions and targets for each task separately and then computes the metrics.\n",
    "* **`MultiTaskTotalMetric(Metric)`**:\n",
    "    *     Similar to `MultiTaskMetric`, but this one calculates the *total* accuracy and ranking metrics across *all* tasks.\n",
    "    *     This gives me an overall performance view.\n",
    "* **`MultiTaskDataset(torch.utils.data.Dataset)`**:\n",
    "    *     This is a custom PyTorch Dataset class that I use to handle datasets with multiple targets (one for each task).\n",
    "    *     It's essential for feeding data to my multi-task models.\n",
    "* **`convert_to_tensor(...)`**:\n",
    "    *     This function is responsible for converting my input data (NumPy arrays) into PyTorch tensors.\n",
    "    *     It also includes checks for NaN and Inf values, which is crucial for numerical stability during training.\n",
    "* **`evaluate_and_visualize(learn, ...)`**:\n",
    "    *     This is a comprehensive function for evaluating my trained model and generating visualizations of the results.\n",
    "    *     It calculates various metrics, confusion matrices, and generates plots to help me analyze the model's performance.\n",
    "* **`get_optimizer(optimizer, lr)`**:\n",
    "    *     This utility function helps me get the appropriate optimizer (SGD, Adam, or AdamW) based on a string identifier.\n",
    "    *     It uses `partial` to pre-configure the optimizers with a learning rate.\n",
    "* **`multitask_loss(preds, targets)`**:\n",
    "    *     This is one of my custom loss functions for multi-task learning.\n",
    "    *     It calculates the combined loss by summing the binary cross-entropy loss for each task.\n",
    "* **`multitask_loss_auto_weighted_bce(preds, targets)`**:\n",
    "    *     Another custom loss function that extends `multitask_loss` by automatically weighting the binary cross-entropy loss for each task based on the class distribution.\n",
    "    *     This helps to address class imbalance.\n",
    "* **`multitask_loss_dynamic_weighted(preds, targets)`**:\n",
    "    *     This loss function is similar to the previous one but uses dynamically updated weights during training.\n",
    "    *     The weights are adjusted based on the loss of each task in the previous step.\n",
    "* **`multitask_loss_loss_proportional(preds, targets, epsilon=1e-8)`**:\n",
    "    *     This loss function weights the loss for each task proportionally to the inverse of the task's loss.\n",
    "    *     This is another strategy to balance the contribution of each task to the overall loss.\n",
    "* **`multitask_splitter(model)`**:\n",
    "    *     This function is used to split the model's parameters into different groups for optimizer configuration (e.g., different learning rates for different parts of the model).\n",
    "    *     It's specific to my `MultiTaskRNN` and `MultiTaskTransformer` models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.callback.core import Callback\n",
    "from fastai.metrics import Metric\n",
    "from fastai.optimizer import OptimWrapper\n",
    "from functools import partial\n",
    "from torch.nn.functional import binary_cross_entropy\n",
    "from torch.optim import SGD, Adam, AdamW\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "\n",
    "# Define initial weights and learning rates outside the loss function\n",
    "lr_large = 0.003\n",
    "lr_small = 0.001\n",
    "binary_cross_entropy = nn.BCELoss()\n",
    "weight_any = torch.tensor(0.5, requires_grad=False)\n",
    "weight_pf = torch.tensor(0.5, requires_grad=False)\n",
    "\n",
    "# Custom callback to print debug information\n",
    "class DebugCallback(Callback):\n",
    "    def after_batch(self):\n",
    "        print(f\"Input: {self.xb}\\nTarget: {self.yb}\\nPredictions: {self.pred}\\nLoss: {self.loss}\")\n",
    "\n",
    "# Custom metric to compute accuracy and ranking metrics for multitask predictions\n",
    "# Computes metrics for each task independently\n",
    "# Args - task_idx: Index of the task in the predictions tuple\n",
    "class MultiTaskMetric(Metric):\n",
    "    def __init__(self, task_idx):\n",
    "        self.task_idx = task_idx  # Index of the task in the predictions tuple\n",
    "        self.epoch_metrics = []  # List to store metrics for each epoch\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # Debugging: Print a message when resetting the predictions and targets\n",
    "        # print(f\"Resetting predictions and targets for task {self.task_idx}...\")\n",
    "        self.preds = np.empty((0, 1))\n",
    "        self.targets = np.empty((0,))\n",
    "\n",
    "    # Accumulate predictions and targets for each batch\n",
    "    def accumulate(self, learn):\n",
    "        # Extract predictions and targets for the specific task\n",
    "        preds, targets = learn.pred[self.task_idx].cpu().numpy(), learn.yb[0][self.task_idx].cpu().numpy()\n",
    "\n",
    "        # Concatenate predictions and targets\n",
    "        self.preds = np.concatenate([self.preds, preds])\n",
    "        self.targets = np.concatenate([self.targets, targets])\n",
    "\n",
    "    @property\n",
    "    # Compute and return the metric values\n",
    "    def value(self):\n",
    "        # Debugging: Print the shapes of self.preds and self.targets\n",
    "        if self.preds.size == 0 or self.targets.size == 0:\n",
    "            print(\"No predictions or targets accumulated. Ensure that the metric is being used correctly.\")\n",
    "            raise ValueError(\"No predictions or targets accumulated. Ensure that the metric is being used correctly.\")\n",
    "\n",
    "        # Calculate metrics and save them for later retrieval\n",
    "        self.metrics = {\n",
    "            'accuracy': self.calculate_accuracy(self.preds, self.targets),\n",
    "            'precision@5': self.calculate_precision_at_k(self.preds, self.targets, k=5),\n",
    "            'precision@10': self.calculate_precision_at_k(self.preds, self.targets, k=10),\n",
    "            'map': self.calculate_map(self.preds, self.targets),\n",
    "            'r_precision': self.calculate_r_precision(self.preds, self.targets),\n",
    "            \"f1_score\": self.calculate_f1_score(self.preds, self.targets)\n",
    "        }\n",
    "    \n",
    "        # Debugging: Log metrics\n",
    "        # print(f\"Metrics for task {self.task_idx}: {self.metrics}\")\n",
    "\n",
    "        # Save metrics for the current epoch\n",
    "        self.epoch_metrics.append(self.metrics)\n",
    "        print(f\"Epoch Metrics: {self.epoch_metrics}\")\n",
    "\n",
    "        # Return the metrics dictionary\n",
    "        return self.metrics\n",
    "\n",
    "    # Validate that targets are binary (0 or 1)\n",
    "    def validate_binary_targets(self, targets, threshold=0.5):\n",
    "        # Check if targets are binary, and binarize if not\n",
    "        if not np.all(np.isin(targets, [0, 1])):\n",
    "            print(\"Non-binary values detected. Binarizing using threshold.\")\n",
    "            targets = (targets > threshold).astype(float)\n",
    "\n",
    "        return targets\n",
    "\n",
    "    # Calculate accuracy\n",
    "    def calculate_accuracy(self, preds, targets):\n",
    "        # Ensure targets are binary\n",
    "        targets = self.validate_binary_targets(targets)\n",
    "\n",
    "        # Convert predictions to binary\n",
    "        preds_binary = (preds > 0.5).astype(float)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        correct = (preds_binary.flatten() == targets.flatten()).sum()\n",
    "        total = targets.size\n",
    "\n",
    "        return float(correct) / float(total) if total > 0 else 0.0\n",
    "\n",
    "    # Calculate precision at k\n",
    "    def calculate_precision_at_k(self, preds, targets, k):\n",
    "        # Ensure targets are binary\n",
    "        targets = self.validate_binary_targets(targets)\n",
    "\n",
    "        # Validate k\n",
    "        if k > len(preds):\n",
    "            print(f\"k ({k}) cannot be greater than the number of predictions ({len(preds)}).\")\n",
    "            raise ValueError(f\"k ({k}) cannot be greater than the number of predictions ({len(preds)}).\")\n",
    "\n",
    "        # Sort predictions in descending order\n",
    "        relevant_indices = np.argsort(preds.flatten())[::-1]\n",
    "        top_k_indices = relevant_indices[:k]\n",
    "\n",
    "        # Get top-k targets\n",
    "        top_k_targets = targets[top_k_indices]\n",
    "\n",
    "        # Calculate precision at k\n",
    "        correct_predictions = np.sum(top_k_targets)\n",
    "        return float(correct_predictions) / float(k) if k > 0 else 0.0\n",
    "\n",
    "    # Calculate mean average precision\n",
    "    def calculate_map(self, preds, targets):\n",
    "        # Ensure targets are binary\n",
    "        targets = self.validate_binary_targets(targets)\n",
    "\n",
    "        # Sort predictions in descending order\n",
    "        relevant_indices = np.argsort(preds.flatten())[::-1]\n",
    "        ranked_targets = targets[relevant_indices]\n",
    "\n",
    "        # Calculate mean average precision\n",
    "        precisions = []\n",
    "        relevant_count = 0\n",
    "\n",
    "        for i, target in enumerate(ranked_targets):\n",
    "            if target == 1:\n",
    "                relevant_count += 1\n",
    "                precisions.append(float(relevant_count) / float(i + 1))\n",
    "\n",
    "        return float(np.mean(precisions)) if precisions else 0.0\n",
    "\n",
    "    # Calculate R-Precision\n",
    "    def calculate_r_precision(self, preds, targets):\n",
    "        # Ensure targets are binary\n",
    "        targets = self.validate_binary_targets(targets)\n",
    "\n",
    "        # Sort predictions in descending order\n",
    "        relevant_indices = np.argsort(preds.flatten())[::-1]\n",
    "        relevant_targets = targets[relevant_indices]\n",
    "\n",
    "        # Calculate R-Precision\n",
    "        R = np.sum(targets)  # Total number of relevant items\n",
    "\n",
    "        if R == 0:\n",
    "            return 0.0\n",
    "\n",
    "        top_r_targets = relevant_targets[:int(R)]\n",
    "        correct_predictions = np.sum(top_r_targets)\n",
    "\n",
    "        return float(correct_predictions) / float(R)\n",
    "\n",
    "   # Calculate F1-score\n",
    "    def calculate_f1_score(self, preds, targets):\n",
    "        # Ensure targets are binary\n",
    "        targets = self.validate_binary_targets(targets)\n",
    "\n",
    "        # Convert predictions to binary\n",
    "        preds_binary = (preds > 0.5).astype(float)\n",
    "\n",
    "        # Calculate True Positives, False Positives, and False Negatives\n",
    "        true_positives = np.sum((preds_binary.flatten() == 1) & (targets.flatten() == 1))\n",
    "        false_positives = np.sum((preds_binary.flatten() == 1) & (targets.flatten() == 0))\n",
    "        false_negatives = np.sum((preds_binary.flatten() == 0) & (targets.flatten() == 1))\n",
    "\n",
    "        # Calculate precision and recall\n",
    "        precision = float(true_positives) / float(true_positives + false_positives) if (true_positives + false_positives) > 0 else 0.0\n",
    "        recall = float(true_positives) / float(true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0.0\n",
    "\n",
    "        # Calculate F1-score\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "        return f1\n",
    "\n",
    "# Custom metric to compute total accuracy and ranking metrics for multitask predictions\n",
    "class MultiTaskTotalMetric(Metric):\n",
    "    def __init__(self):\n",
    "        self.epoch_metrics = []  # List to store metrics for each epoch\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # Debugging: Print a message when resetting the predictions and targets\n",
    "        # print(f\"Resetting predictions and targets for task total...\")\n",
    "        self.preds_any = np.empty((0, 1))\n",
    "        self.preds_pf = np.empty((0, 1))\n",
    "        self.targets_any = np.empty((0,))\n",
    "        self.targets_pf = np.empty((0,))\n",
    "\n",
    "    # Accumulate predictions and targets for each batch\n",
    "    def accumulate(self, learn):\n",
    "        # Extract predictions and targets for both tasks\n",
    "        preds_any, preds_pf = learn.pred[0].cpu().numpy(), learn.pred[1].cpu().numpy()\n",
    "        targets_any, targets_pf = learn.yb[0][0].cpu().numpy(), learn.yb[0][1].cpu().numpy()\n",
    "\n",
    "        self.preds_any = np.concatenate([self.preds_any, preds_any])\n",
    "        self.preds_pf = np.concatenate([self.preds_pf, preds_pf])\n",
    "        self.targets_any = np.concatenate([self.targets_any, targets_any])\n",
    "        self.targets_pf = np.concatenate([self.targets_pf, targets_pf])\n",
    "\n",
    "    @property\n",
    "    # Compute and return the metric values\n",
    "    def value(self):\n",
    "        # Ensure predictions and targets are not empty\n",
    "        if self.preds_any.size == 0 or self.preds_pf.size == 0 or self.targets_any.size == 0 or self.targets_pf.size == 0:\n",
    "            print(\"No predictions or targets accumulated. Ensure that the metric is being used correctly.\")\n",
    "            raise ValueError(\"No predictions or targets accumulated. Ensure that the metric is being used correctly.\")\n",
    "\n",
    "        total_preds = np.concatenate([self.preds_any, self.preds_pf])\n",
    "        total_targets = np.concatenate([self.targets_any, self.targets_pf])\n",
    "\n",
    "        # Debugging: Log shapes of predictions and targets\n",
    "        # print(f\"Total predictions shape: {total_preds.shape}, Total targets shape: {total_targets.shape}\")\n",
    "\n",
    "        # Calculate metrics and save them for later retrieval\n",
    "        self.metrics = {\n",
    "            'accuracy': self.calculate_accuracy(total_preds, total_targets),\n",
    "            'precision@5': self.calculate_precision_at_k(total_preds, total_targets, k=5),\n",
    "            'precision@10': self.calculate_precision_at_k(total_preds, total_targets, k=10),\n",
    "            'map': self.calculate_map(total_preds, total_targets),\n",
    "            'r_precision': self.calculate_r_precision(total_preds, total_targets),\n",
    "            \"f1_score\": self.calculate_f1_score(total_preds, total_targets), \n",
    "        }\n",
    "\n",
    "        # Debugging: Log metrics\n",
    "        # print(f\"Metrics: {self.metrics}\")\n",
    "\n",
    "        # Save metrics for the current epoch\n",
    "        self.epoch_metrics.append(self.metrics)\n",
    "        print(f\"Epoch Metrics: {self.epoch_metrics}\")\n",
    "\n",
    "        # Return the metrics dictionary\n",
    "        return self.metrics\n",
    "\n",
    "    # Validate that targets are binary (0 or 1)\n",
    "    def validate_binary_targets(self, targets, threshold=0.5):\n",
    "        # Check if targets are binary, and binarize if not\n",
    "        if not np.all(np.isin(targets, [0, 1])):\n",
    "            print(\"Non-binary values detected. Binarizing using threshold.\")\n",
    "            targets = (targets > threshold).astype(float)\n",
    "\n",
    "        return targets\n",
    "\n",
    "    # Calculate accuracy\n",
    "    def calculate_accuracy(self, preds, targets):\n",
    "        # Ensure targets are binary\n",
    "        targets = self.validate_binary_targets(targets)\n",
    "\n",
    "        # Convert predictions to binary\n",
    "        preds_binary = (preds > 0.5).astype(float)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        correct = (preds_binary.flatten() == targets.flatten()).sum()\n",
    "        total = targets.size\n",
    "\n",
    "        # Normalize accuracy\n",
    "        return float(correct) / float(total) if total > 0 else 0.0\n",
    "\n",
    "    # Calculate precision at k\n",
    "    def calculate_precision_at_k(self, preds, targets, k):\n",
    "        # Ensure targets are binary\n",
    "        targets = self.validate_binary_targets(targets)\n",
    "\n",
    "        # Validate k\n",
    "        if k > len(preds):\n",
    "            print(f\"k ({k}) cannot be greater than the number of predictions ({len(preds)}).\")\n",
    "            raise ValueError(f\"k ({k}) cannot be greater than the number of predictions ({len(preds)}).\")\n",
    "\n",
    "        # Sort predictions in descending order\n",
    "        relevant_indices = np.argsort(preds.flatten())[::-1]\n",
    "        top_k_indices = relevant_indices[:k]\n",
    "\n",
    "        # Get top-k targets\n",
    "        top_k_targets = targets[top_k_indices]\n",
    "\n",
    "        # Calculate precision at k\n",
    "        correct_predictions = np.sum(top_k_targets)\n",
    "        return float(correct_predictions) / float(k) if k > 0 else 0.0\n",
    "\n",
    "    # Calculate mean average precision\n",
    "    def calculate_map(self, preds, targets):\n",
    "        # Ensure targets are binary\n",
    "        targets = self.validate_binary_targets(targets)\n",
    "\n",
    "        # Sort predictions in descending order\n",
    "        relevant_indices = np.argsort(preds.flatten())[::-1]\n",
    "        ranked_targets = targets[relevant_indices]\n",
    "\n",
    "        # Calculate mean average precision\n",
    "        precisions = []\n",
    "        relevant_count = 0\n",
    "\n",
    "        for i, target in enumerate(ranked_targets):\n",
    "            if target == 1:\n",
    "                relevant_count += 1\n",
    "                precisions.append(float(relevant_count) / float(i + 1))\n",
    "\n",
    "        return float(np.mean(precisions)) if precisions else 0.0\n",
    "\n",
    "    # Calculate R-Precision\n",
    "    def calculate_r_precision(self, preds, targets):\n",
    "        # Ensure targets are binary\n",
    "        targets = self.validate_binary_targets(targets)\n",
    "\n",
    "        # Sort predictions in descending order\n",
    "        relevant_indices = np.argsort(preds.flatten())[::-1]\n",
    "        relevant_targets = targets[relevant_indices]\n",
    "\n",
    "        # Calculate R-Precision\n",
    "        R = np.sum(targets)  # Total number of relevant items\n",
    "\n",
    "        if R == 0:\n",
    "            return 0.0\n",
    "\n",
    "        top_r_targets = relevant_targets[:int(R)]\n",
    "        correct_predictions = np.sum(top_r_targets)\n",
    "\n",
    "        return float(correct_predictions) / float(R)\n",
    "    \n",
    "    # Calculate F1-score\n",
    "    def calculate_f1_score(self, preds, targets):\n",
    "        # Ensure targets are binary\n",
    "        targets = self.validate_binary_targets(targets)\n",
    "\n",
    "        # Convert predictions to binary\n",
    "        preds_binary = (preds > 0.5).astype(float)\n",
    "\n",
    "        #   Calculate True Positives, False Positives, and False Negatives\n",
    "        true_positives = np.sum((preds_binary.flatten() == 1) & (targets.flatten() == 1))\n",
    "        false_positives = np.sum((preds_binary.flatten() == 1) & (targets.flatten() == 0))\n",
    "        false_negatives = np.sum((preds_binary.flatten() == 0) & (targets.flatten() == 1))\n",
    "\n",
    "        # Calculate precision and recall\n",
    "        precision = float(true_positives)/ float(true_positives + false_positives) if (true_positives + false_positives) > 0 else 0.0\n",
    "        recall = float(true_positives)/ float(true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0.0\n",
    "\n",
    "        # Calculate F1-score\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "        return f1\n",
    "\n",
    "# Custom dataset class to handle multiple targets\n",
    "# This class is used to create a dataset that returns multiple targets for each input sample\n",
    "# The dataset is used to train a model that predicts multiple targets simultaneously\n",
    "class MultiTaskDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, targets_any, targets_pf):\n",
    "        self.inputs = inputs\n",
    "        self.targets_any = targets_any\n",
    "        self.targets_pf = targets_pf\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], (self.targets_any[idx], self.targets_pf[idx])\n",
    "\n",
    "# Custom model for multitask learning and log for debugging\n",
    "def convert_to_tensor(X_train, X_val, y_train_any, y_train_pf, y_val_any, y_val_pf):\n",
    "    # Convert everything to float for PyTorch tensors\n",
    "    y_train_any_numeric = y_train_any.astype(float)\n",
    "    y_train_pf_numeric = y_train_pf.astype(float)\n",
    "    y_val_any_numeric = y_val_any.astype(float)\n",
    "    y_val_pf_numeric = y_val_pf.astype(float)\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_any_tensor = torch.tensor(y_train_any_numeric, dtype=torch.float32)\n",
    "    y_train_pf_tensor = torch.tensor(y_train_pf_numeric, dtype=torch.float32)\n",
    "\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val_any_tensor = torch.tensor(y_val_any_numeric, dtype=torch.float32)\n",
    "    y_val_pf_tensor = torch.tensor(y_val_pf_numeric, dtype=torch.float32)\n",
    "\n",
    "    # Check for NaN or Inf in the training and validation data\n",
    "    print(\"Checking training data for NaN and inf...\")\n",
    "    X_train_nan = torch.isnan(X_train_tensor).any()\n",
    "    y_train_any_nan = torch.isnan(y_train_any_tensor).any()\n",
    "    y_train_pf_nan = torch.isnan(y_train_pf_tensor).any()\n",
    "\n",
    "    if X_train_nan or y_train_any_nan or y_train_pf_nan:\n",
    "        print(f\"NaN detected in training data! X_train: {X_train_nan} | y_train_any: {y_train_any_nan} | y_train_pf {y_train_pf_nan}\")\n",
    "    \n",
    "    X_train_inf = torch.isinf(X_train_tensor).any()\n",
    "    y_train_any_inf = torch.isinf(y_train_any_tensor).any()\n",
    "    y_train_pf_inf = torch.isinf(y_train_pf_tensor).any()\n",
    "\n",
    "    if X_train_inf or y_train_any_inf or y_train_pf_inf:\n",
    "        print(f\"Inf detected in training data! X_train: {X_train_inf} | y_train_any: {y_train_any_inf} | y_train_pf {y_train_pf_inf}\")\n",
    "\n",
    "    print(\"Checking validation data for NaN and inf...\")\n",
    "\n",
    "    X_val_nan = torch.isnan(X_val_tensor).any()\n",
    "    y_val_any_nan = torch.isnan(y_val_any_tensor).any()\n",
    "    y_val_pf_nan = torch.isnan(y_val_pf_tensor).any()\n",
    "\n",
    "    if X_val_nan or y_val_any_nan or y_val_pf_nan:\n",
    "        print(f\"NaN detected in validation data! X_val: {X_val_nan} | y_val_any: {y_val_any_nan} | y_val_pf {y_val_pf_nan}\")\n",
    "\n",
    "    X_val_inf = torch.isinf(X_val_tensor).any()\n",
    "    y_val_any_inf = torch.isinf(y_val_any_tensor).any()\n",
    "    y_val_pf_inf = torch.isinf(y_val_pf_tensor).any()\n",
    "\n",
    "    if X_val_inf or y_val_any_inf or y_val_pf_inf:\n",
    "        print(f\"Inf detected in validation data! X_val: {X_val_inf} | y_val_any: {y_val_any_inf} | y_val_pf {y_val_pf_inf}\")\n",
    "\n",
    "    # Check shapes\n",
    "    print(f\"X_train shape: {X_train_tensor.shape} | y_train_any shape: {y_train_any_tensor.shape} | y_train_pf shape: {y_train_pf_tensor.shape}\")\n",
    "    print(f\"X_val shape: {X_val_tensor.shape} | y_val_any shape: {y_val_any_tensor.shape} | y_val_pf shape: {y_val_pf_tensor.shape}\")\n",
    "    \n",
    "    return X_train_tensor, X_val_tensor, y_train_any_tensor, y_train_pf_tensor, y_val_any_tensor, y_val_pf_tensor\n",
    "\n",
    "# Custom function to evaluate and visualize the model\n",
    "# This function evaluates the model on the validation set and visualizes the results\n",
    "#\n",
    "# Args:\n",
    "#   learn (Learner): The FastAI Learner object containing the trained model.\n",
    "#   task_names (list): List of task names for multitask learning (default: [\"Any\", \"PF\"]).\n",
    "#   threshold (float): Threshold for converting probabilities to binary predictions (default: 0.5).\n",
    "def evaluate_and_visualize(learn, task_names=[\"Any\", \"PF\"], threshold=0.5, epochs=100):\n",
    "    # Get predictions and targets for the validation set\n",
    "    vld_preds, vld_targets = learn.get_preds(ds_idx=1) \n",
    "\n",
    "    print(f\"Validation predictions: {vld_preds} | Validation targets: {vld_preds}\")\n",
    "    print(\"-----------------------------------------------------------------------------------------\")\n",
    "\n",
    "    # Unpack predictions for each task\n",
    "    pred_any, pred_pf = vld_preds\n",
    "    target_any, target_pf = vld_targets\n",
    "\n",
    "    # Combine the targets for all tasks into a single tensor\n",
    "    total_targets = torch.cat([target_any.flatten(), target_pf.flatten()])\n",
    "    total_preds = torch.cat([pred_any.flatten(), pred_pf.flatten()])\n",
    "\n",
    "    # Check if the validation set is empty\n",
    "    if len(target_any) == 0 or len(target_pf) == 0:\n",
    "        print(\"Validation set is empty. Skipping evaluation.\")\n",
    "        return\n",
    "\n",
    "    # Convert probabilities to binary predictions using the threshold\n",
    "    pred_any_binary = (pred_any > threshold).float()\n",
    "    pred_pf_binary = (pred_pf > threshold).float()\n",
    "    target_any_binary = (target_any > threshold).float()\n",
    "    target_pf_binary = (target_pf > threshold).float()\n",
    "\n",
    "    # Find indices of incorrect and correct predictions for each task\n",
    "    incorrect_any = torch.where((target_any_binary == pred_any_binary) == False)[0]\n",
    "    correct_any = torch.where((target_any_binary == pred_any_binary) == True)[0]\n",
    "\n",
    "    incorrect_pf = torch.where((target_pf_binary == pred_pf_binary) == False)[0]\n",
    "    correct_pf = torch.where((target_pf_binary == pred_pf_binary) == True)[0]\n",
    "\n",
    "    # Calculate validation loss and metrics\n",
    "    vld_loss, *metrics = learn.validate()\n",
    "\n",
    "    # Extract accuracy for each task\n",
    "    task_accuracies = metrics[:len(task_names)]  # Extract accuracy for each task\n",
    "\n",
    "    # Print results\n",
    "    print(\"-----------------------------------------------------------------------------------------\")\n",
    "    print(f\"Validation Loss: {vld_loss:.5f}\")\n",
    "    print(\"-----------------------------------------------------------------------------------------\")\n",
    "    print(f\"Number of test data samples misclassified (Any Task): {len(incorrect_any):,}\")\n",
    "    print(f\"Number of test data samples correctly classified (Any Task): {len(correct_any):,}\")\n",
    "    print(f\"Number of test data samples misclassified (PF Task): {len(incorrect_pf):,}\")\n",
    "    print(f\"Number of test data samples correctly classified (PF Task): {len(correct_pf):,}\")\n",
    "    print(\"-----------------------------------------------------------------------------------------\")\n",
    "    print(f\"Shape of validation targets: {len(total_targets.shape):,}\")\n",
    "    print(f\"Total Number of validation targets: {len(total_targets):,}\")\n",
    "    print(f\"Shape of validation predictions: {len(total_preds.shape):,}\")\n",
    "    print(f\"Total Number of validation predictions: {len(total_preds):,}\")\n",
    "    print(\"-----------------------------------------------------------------------------------------\")\n",
    "\n",
    "    # Prepare data for visualization\n",
    "    # Compute confusion matrices\n",
    "    target_any_binary_int = target_any_binary.int()\n",
    "    pred_any_binary_int = pred_any_binary.int()\n",
    "    conf_matrix_any = confusion_matrix(target_any_binary_int.cpu(), pred_any_binary_int.cpu())\n",
    "\n",
    "    target_pf_binary_int = target_pf_binary.int()\n",
    "    pred_pf_binary_int = pred_pf_binary.int()\n",
    "    conf_matrix_pf = confusion_matrix(target_pf_binary_int.cpu(), pred_pf_binary_int.cpu())\n",
    "\n",
    "    # Retrieve metrics from MultiTaskMetric and MultiTaskTotalMetric\n",
    "    # Metrics for task 0 (Any)\n",
    "    metrics_task_any = learn.metrics[0].metrics\n",
    "\n",
    "    # Iterate over the metrics dictionary and print each metric\n",
    "    for metric_name, metric_value in metrics_task_any.items():\n",
    "        print(f\"(Any {metric_name} Metric): {metric_value:.5f}\")\n",
    "\n",
    "    # Metrics for task 1 (PF)\n",
    "    metrics_task_pf = learn.metrics[1].metrics\n",
    "\n",
    "    # Iterate over the metrics dictionary and print each metric\n",
    "    for metric_name, metric_value in metrics_task_pf.items():\n",
    "        print(f\"(PF {metric_name} Metric): {metric_value:.5f}\")\n",
    "\n",
    "    # Combined metrics for all tasks\n",
    "    metrics_total = learn.metrics[2].metrics\n",
    "\n",
    "    # Iterate over the metrics dictionary and print each metric\n",
    "    for metric_name, metric_value in metrics_total.items():\n",
    "        print(f\"(Total {metric_name} Metric): {metric_value:.5f}\")\n",
    "\n",
    "    # Extract metrics for visualization\n",
    "    accuracy_any = metrics_task_any['accuracy']\n",
    "    accuracy_pf = metrics_task_pf['accuracy']\n",
    "    accuracy_total = metrics_total['accuracy']\n",
    "\n",
    "    precision_at_5_any = metrics_task_any['precision@5']\n",
    "    precision_at_5_pf = metrics_task_pf['precision@5']\n",
    "    precision_at_5_total = metrics_total['precision@5']\n",
    "\n",
    "    precision_at_10_any = metrics_task_any['precision@10']\n",
    "    precision_at_10_pf = metrics_task_pf['precision@10']\n",
    "    precision_at_10_total = metrics_total['precision@10']\n",
    "\n",
    "    map_any = metrics_task_any['map']\n",
    "    map_pf = metrics_task_pf['map']\n",
    "    map_total = metrics_total['map']\n",
    "\n",
    "    r_precision_any = metrics_task_any['r_precision']\n",
    "    r_precision_pf = metrics_task_pf['r_precision']\n",
    "    r_precision_total = metrics_total['r_precision']\n",
    "\n",
    "    # Print metrics\n",
    "    print(\"-----------------------------------------------------------------------------------------\")\n",
    "    print(f\"Learn.Recorder.Values: {learn.recorder.values}\")\n",
    "    print(f\"Metrics for Task 0 (Any): {metrics_task_any}\")\n",
    "    print(f\"Metrics for Task 1 (PF): {metrics_task_pf}\")\n",
    "    print(f\"Combined Metrics: {metrics_total}\")\n",
    "    print(\"-----------------------------------------------------------------------------------------\")\n",
    "\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(20, 15))\n",
    "    fig.suptitle(\"Model Validation Visualizations\", fontsize=24, y=1.02)\n",
    "\n",
    "    # Define a color palette using 'magma' for a purple-blue range\n",
    "    palette = sns.color_palette(\"twilight\", n_colors=4)\n",
    "\n",
    "    # Plot 1: Correct vs. Incorrect Predictions (Both Tasks)\n",
    "    # Prepare data for the grouped bar chart\n",
    "    total_any = conf_matrix_any.sum()\n",
    "    correct_any = (conf_matrix_any[0, 0] + conf_matrix_any[1, 1]) / total_any * 100\n",
    "    incorrect_any = (conf_matrix_any[0, 1] + conf_matrix_any[1, 0]) / total_any * 100\n",
    "\n",
    "    total_pf = conf_matrix_pf.sum()\n",
    "    correct_pf = (conf_matrix_pf[0, 0] + conf_matrix_pf[1, 1]) / total_pf * 100\n",
    "    incorrect_pf = (conf_matrix_pf[0, 1] + conf_matrix_pf[1, 0]) / total_pf * 100\n",
    "\n",
    "    tasks = {\n",
    "        'Two': ['Any', 'PF'],\n",
    "        'Three': ['Any', 'PF', \"Total\"],\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        'Task': ['Any', 'Any', 'PF', 'PF'],\n",
    "        'Prediction': ['Correct', 'Incorrect', 'Correct', 'Incorrect'],\n",
    "        'Percentage': [correct_any, incorrect_any, correct_pf, incorrect_pf]\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Create the grouped bar chart\n",
    "    sns.barplot(x='Task', y='Percentage', hue='Prediction', data=df, ax=axes[0, 0], palette=palette[:2])\n",
    "    axes[0, 0].set_title(\"Correct vs. Incorrect Predictions (Any and PF Tasks)\")\n",
    "    axes[0, 0].set_ylabel(\"Percentage\")\n",
    "    axes[0, 0].legend(title='Prediction')  # Add a legend with a title\n",
    "\n",
    "    # Plot 2: F1 Score Comparison\n",
    "    f1_any = metrics_task_any['f1_score']\n",
    "    f1_pf = metrics_task_pf['f1_score']\n",
    "    f1_total = metrics_total['f1_score']\n",
    "\n",
    "    # Data preparation for F1 score chart\n",
    "    f1_data = {\n",
    "        'Category': ['Any', 'PF', 'Total'],\n",
    "        'F1 Score': [f1_any, f1_pf, f1_total]\n",
    "    }\n",
    "\n",
    "    df_f1 = pd.DataFrame(f1_data)\n",
    "\n",
    "    # Create the F1 score bar chart\n",
    "    sns.barplot(x='Category', y='F1 Score', hue='Category', data=df_f1, ax=axes[0, 1], palette=palette[:3])  \n",
    "    axes[0, 1].set_title(\"F1 Score Comparison\")\n",
    "    axes[0, 1].set_ylabel(\"F1 Score\")\n",
    "    axes[0, 1].set_ylim(0, 1)  # Set y-axis limit between 0 and 1 for F1 score\n",
    "\n",
    "    # Add labels to the bars\n",
    "    for index, row in df_f1.iterrows():\n",
    "        axes[0, 1].text(row.name, row['F1 Score'], round(row['F1 Score'], 2), color='black', ha=\"center\", va=\"bottom\")\n",
    "\n",
    "    # Plot 3: Task-Specific Accuracy\n",
    "    sns.barplot(x=tasks['Two'], y=[accuracy_any, accuracy_pf], hue=tasks['Two'], ax=axes[0, 2], palette=palette[:2])\n",
    "    axes[0, 2].set_title(\"Task-Specific Accuracy Comparison\")\n",
    "    axes[0, 2].set_ylabel(\"Accuracy\")\n",
    "\n",
    "    # Plot 4: Confusion Matrix (Any Task)\n",
    "    conf_matrix_any_normalized = conf_matrix_any.astype('float') / conf_matrix_any.sum(axis=1)[:, np.newaxis] * 100\n",
    "\n",
    "    # Create annotations with both percentages and raw numbers\n",
    "    annot_any = np.array([[f\"{conf_matrix_any_normalized[i, j]:.1f}% ({conf_matrix_any[i, j]})\" for j in range(conf_matrix_any.shape[1])] for i in range(conf_matrix_any.shape[0])])\n",
    "\n",
    "    sns.heatmap(conf_matrix_any_normalized,\n",
    "                annot=annot_any, fmt=\"\", cmap=sns.blend_palette([\"white\", \"purple\"], as_cmap=True),  # Use custom annotations, fmt=\"\" disables default formatting\n",
    "                xticklabels=[\"Not Check-Worthy\", \"Check-Worthy\"],\n",
    "                yticklabels=[\"Not Check-Worthy\", \"Check-Worthy\"],\n",
    "                ax=axes[1, 0])\n",
    "    axes[1, 0].set_title(\"Normalized Confusion Matrix (Any Task) [%]\")\n",
    "    axes[1, 0].set_xlabel(\"Predicted\")\n",
    "    axes[1, 0].set_ylabel(\"Actual\")\n",
    "\n",
    "    # Plot 5: Confusion Matrix (PF Task)\n",
    "    conf_matrix_pf_normalized = conf_matrix_pf.astype('float') / conf_matrix_pf.sum(axis=1)[:, np.newaxis] * 100\n",
    "\n",
    "    # Create annotations with both percentages and raw numbers\n",
    "    annot_pf = np.array([[f\"{conf_matrix_pf_normalized[i, j]:.1f}% ({conf_matrix_pf[i, j]})\" for j in range(conf_matrix_pf.shape[1])] for i in range(conf_matrix_pf.shape[0])])\n",
    "\n",
    "    sns.heatmap(conf_matrix_pf_normalized,\n",
    "                annot=annot_pf, fmt=\"\", cmap=sns.blend_palette([\"white\", \"purple\"], as_cmap=True),  # Use custom annotations, fmt=\"\" disables default formatting\n",
    "                xticklabels=[\"Not Check-Worthy\", \"Check-Worthy\"],\n",
    "                yticklabels=[\"Not Check-Worthy\", \"Check-Worthy\"],\n",
    "                ax=axes[1, 1])\n",
    "    axes[1, 1].set_title(\"Normalized Confusion Matrix (PF Task) [%]\")\n",
    "    axes[1, 1].set_xlabel(\"Predicted\")\n",
    "    axes[1, 1].set_ylabel(\"Actual\")\n",
    "\n",
    "    # Plot 6: Precision@K Comparison\n",
    "    # Prepare data for grouped bar chart\n",
    "    data = {\n",
    "        'Category': ['Any', 'PF', 'Total', 'Any', 'PF', 'Total'],\n",
    "        'Precision': [precision_at_5_any, precision_at_5_pf, precision_at_5_total, precision_at_10_any, precision_at_10_pf, precision_at_10_total],\n",
    "        'K': ['Precision@5', 'Precision@5', 'Precision@5', 'Precision@10', 'Precision@10', 'Precision@10']\n",
    "    }\n",
    "\n",
    "    df_pk = pd.DataFrame(data)\n",
    "\n",
    "    sns.barplot(x='Category', y='Precision', hue='K', data=df_pk, ax=axes[1, 2], palette=sns.color_palette(['slateblue', 'darkblue']))\n",
    "    axes[1, 2].set_title(\"Precision@K Comparison\")\n",
    "    axes[1, 2].set_ylabel(\"Precision\")\n",
    "    axes[1, 2].legend(title='K')\n",
    "\n",
    "    # Plot 7: F1 Score Over Epochs\n",
    "    # Extract F1 Score values over epochs - Adjust this based on how your 'learn' object stores epoch metrics\n",
    "    task_any_f1 = [epoch['f1_score'] for epoch in learn.metrics[0].epoch_metrics]  # Task 0 (Any)\n",
    "    task_pf_f1 = [epoch['f1_score'] for epoch in learn.metrics[1].epoch_metrics]  # Task 1 (PF)\n",
    "    task_total_f1 = [epoch['f1_score'] for epoch in learn.metrics[2].epoch_metrics]  # Combined\n",
    "\n",
    "    sns.lineplot(x=range(len(task_any_f1)), y=task_any_f1, label=\"Any F1 Score\", ax=axes[2, 0], color='purple')\n",
    "    sns.lineplot(x=range(len(task_pf_f1)), y=task_pf_f1, label=\"PF F1 Score\", ax=axes[2, 0], color='blue')\n",
    "    sns.lineplot(x=range(len(task_total_f1)), y=task_total_f1, label=\"Total F1 Score\", ax=axes[2, 0], color='black')\n",
    "    axes[2, 0].set_title(\"F1 Score Over Epochs\")\n",
    "    axes[2, 0].set_xlabel(\"Epochs\")\n",
    "    axes[2, 0].set_ylabel(\"F1 Score\")\n",
    "    axes[2, 0].legend()\n",
    "\n",
    "    # Plot 8: R-Precision Comparison\n",
    "    sns.barplot(x=tasks['Three'], y=[r_precision_any, r_precision_pf, r_precision_total], hue=tasks['Three'], ax=axes[2, 1], palette=palette[:3])\n",
    "    axes[2, 1].set_title(\"R-Precision Comparison\")\n",
    "    axes[2, 1].set_ylabel(\"R-Precision\")\n",
    "\n",
    "    # Plot 9: Mean Average Precision (MAP) Comparison\n",
    "    sns.barplot(x=tasks['Three'], y=[map_any, map_pf, map_total], hue=tasks['Three'], ax=axes[2, 2], palette=palette[:3])\n",
    "    axes[2, 2].set_title(\"Mean Average Precision (MAP) Comparison\")\n",
    "    axes[2, 2].set_ylabel(\"MAP\")\n",
    "\n",
    "    # Adjust layout for better spacing\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure to a file\n",
    "    model_name = str(learn.model).split('(')[0]\n",
    "    model_number = random.randint(1, 999999) \n",
    "    plt.savefig(f'{model_name}_{model_number}.png')\n",
    "\n",
    "    # Show the grid of plots\n",
    "    plt.show()\n",
    "\n",
    "# Function to get the optimizer based on the optimizer name\n",
    "# This function returns a partial function that can be used to create an optimizer for fastai learner\n",
    "def get_optimizer(optimizer, lr):\n",
    "    if optimizer == 'SGD':\n",
    "        return partial(OptimWrapper, opt=SGD, lr=lr, momentum=0.9, nesterov=True)\n",
    "    elif optimizer == 'Adam':\n",
    "        return partial(OptimWrapper, opt=Adam, lr=lr)\n",
    "    elif optimizer == 'AdamW':\n",
    "        return partial(OptimWrapper, opt=AdamW, lr=lr)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimizer: {optimizer}\")\n",
    "    \n",
    "# # Custom loss function for multitask learning\n",
    "# # This function computes the combined loss for multiple tasks\n",
    "# # The loss for each task is computed using binary cross-entropy loss    \n",
    "def multitask_loss(preds, targets):\n",
    "    # Debugging: Print predictions and targets\n",
    "    # print(f\"Preds: {preds}\")\n",
    "    # print(f\"Targets: {targets}\")\n",
    "\n",
    "    # Unpack predictions and targets\n",
    "    pred_any, pred_pf = preds  # Predictions for each task\n",
    "    y_any, y_pf = targets      # Ground truth labels for each task\n",
    "\n",
    "    # Reshape targets to match predictions\n",
    "    y_any = y_any.view(-1, 1)\n",
    "    y_pf = y_pf.view(-1, 1)\n",
    "\n",
    "    # Debugging: Check for NaN, Inf, and shape mismatches\n",
    "    if torch.isnan(pred_any).any() or torch.isnan(pred_pf).any():\n",
    "        print(\"NaN detected in predictions!\")\n",
    "    if torch.isnan(y_any).any() or torch.isnan(y_pf).any():\n",
    "        print(\"NaN detected in targets!\")\n",
    "    if pred_any.shape != y_any.shape or pred_pf.shape != y_pf.shape:\n",
    "        print(f\"Shape mismatch: pred_any {pred_any.shape}, y_any {y_any.shape}, pred_pf {pred_pf.shape}, y_pf {y_pf.shape}\")\n",
    "\n",
    "    # Ensure predictions and targets are on the same device\n",
    "    pred_any, pred_pf = pred_any.to(y_any.device), pred_pf.to(y_pf.device)\n",
    "\n",
    "    # Compute binary cross-entropy loss for each task\n",
    "    loss_any = binary_cross_entropy(pred_any, y_any)\n",
    "    loss_pf = binary_cross_entropy(pred_pf, y_pf)\n",
    "\n",
    "    # Combine the losses (you can adjust initial learning rates if needed)\n",
    "    combined_loss = loss_any + loss_pf\n",
    "\n",
    "    print(f\"Combined loss: {combined_loss}\")\n",
    "    return combined_loss\n",
    "\n",
    "# Custom loss function for multitask learning using weighted binary cross-entropy loss\n",
    "# Computes a combined loss for multitask learning using weighted binary cross-entropy with automatic weight calculation for each task.\n",
    "def multitask_loss_auto_weighted_bce(preds, targets):\n",
    "    # Unpack predictions and targets\n",
    "    pred_any, pred_pf = preds\n",
    "    y_any, y_pf = targets\n",
    "\n",
    "    # Reshape targets to match predictions\n",
    "    y_any = y_any.view(-1, 1)\n",
    "    y_pf = y_pf.view(-1, 1)\n",
    "\n",
    "    # Debugging: Check for NaN, Inf, and shape mismatches\n",
    "    if torch.isnan(pred_any).any() or torch.isnan(pred_pf).any():\n",
    "        print(\"NaN detected in predictions!\")\n",
    "    if torch.isnan(y_any).any() or torch.isnan(y_pf).any():\n",
    "        print(\"NaN detected in targets!\")\n",
    "    if pred_any.shape != y_any.shape or pred_pf.shape != y_pf.shape:\n",
    "        print(f\"Shape mismatch: pred_any {pred_any.shape}, y_any {y_any.shape}, pred_pf {pred_pf.shape}, y_pf {y_pf.shape}\")\n",
    "\n",
    "    # Ensure predictions and targets are on the same device\n",
    "    pred_any, pred_pf = pred_any.to(y_any.device), pred_pf.to(y_pf.device)\n",
    "\n",
    "    # Custom helper function for the loss function for multitask learning using weighted binary cross-entropy loss\n",
    "    # Computes weighted binary cross-entropy loss, calculating weights automatically based on the class distribution in the target. \n",
    "    def weighted_binary_cross_entropy(preds, target):\n",
    "        with torch.no_grad():\n",
    "            pos_count = torch.sum(target)\n",
    "            neg_count = target.numel() - pos_count\n",
    "\n",
    "            # Avoid division by zero\n",
    "            pos_weight = neg_count / (pos_count + 1e-5)\n",
    "            neg_weight = pos_count / (neg_count + 1e-5)\n",
    "\n",
    "            weights = torch.tensor([neg_weight, pos_weight]).to(target.device)\n",
    "\n",
    "        bce = binary_cross_entropy(preds, target)\n",
    "        \n",
    "        # Apply the weighted binary cross-entropy loss to account for class imbalance\n",
    "        # The loss is weighted based on the class distribution in the target of the positive and negaive classes\n",
    "        weighted_bce = weights[1] * target * bce + weights[0] * (1 - target) * bce\n",
    "\n",
    "        return torch.mean(weighted_bce)\n",
    "\n",
    "    # Compute weighted BCE loss for each task with automatic weights\n",
    "    loss_any = weighted_binary_cross_entropy(pred_any, y_any)\n",
    "    loss_pf = weighted_binary_cross_entropy(pred_pf, y_pf)\n",
    "\n",
    "    # Combine the losses (you can adjust initial learning rates if needed)\n",
    "    combined_loss = loss_any + loss_pf\n",
    "\n",
    "    print(f\"Combined loss: {combined_loss}\")\n",
    "    return combined_loss\n",
    "\n",
    "# Custom loss function for multitask learning\n",
    "# This function computes the combined loss for multiple tasks\n",
    "# The loss for each task is computed using binary cross-entropy loss    \n",
    "def multitask_loss_dynamic_weighted(preds, targets):\n",
    "    global weight_any, weight_pf # use global weights\n",
    "    # Debugging: Print predictions and targets\n",
    "    # print(f\"Preds: {preds}\")\n",
    "    # print(f\"Targets: {targets}\")\n",
    "\n",
    "    # Unpack predictions and targets\n",
    "    pred_any, pred_pf = preds  # Predictions for each task\n",
    "    y_any, y_pf = targets      # Ground truth labels for each task\n",
    "\n",
    "    # Reshape targets to match predictions\n",
    "    y_any = y_any.view(-1, 1)\n",
    "    y_pf = y_pf.view(-1, 1)\n",
    "\n",
    "    # Debugging: Check for NaN, Inf, and shape mismatches\n",
    "    if torch.isnan(pred_any).any() or torch.isnan(pred_pf).any():\n",
    "        print(\"NaN detected in predictions!\")\n",
    "    if torch.isnan(y_any).any() or torch.isnan(y_pf).any():\n",
    "        print(\"NaN detected in targets!\")\n",
    "    if pred_any.shape != y_any.shape or pred_pf.shape != y_pf.shape:\n",
    "        print(f\"Shape mismatch: pred_any {pred_any.shape}, y_any {y_any.shape}, pred_pf {pred_pf.shape}, y_pf {y_pf.shape}\")\n",
    "\n",
    "    # Ensure predictions and targets are on the same device\n",
    "    pred_any, pred_pf = pred_any.to(y_any.device), pred_pf.to(y_pf.device)\n",
    "\n",
    "    # Compute binary cross-entropy loss for each task\n",
    "    loss_any = binary_cross_entropy(pred_any, y_any)\n",
    "    loss_pf = binary_cross_entropy(pred_pf, y_pf)\n",
    "\n",
    "    def update_weights(loss_any, loss_pf):\n",
    "        global weight_any, weight_pf\n",
    "        if loss_any > loss_pf:\n",
    "            weight_any += lr_large\n",
    "            weight_pf -= lr_large\n",
    "        else:\n",
    "            weight_any -= lr_small\n",
    "            weight_pf += lr_small\n",
    "\n",
    "        # Ensure weights stay within [0, 1] and normalize them\n",
    "        weight_any = torch.clamp(weight_any, 0, 1)\n",
    "        weight_pf = torch.clamp(weight_pf, 0, 1)\n",
    "\n",
    "        # Normalize to make sure they sum to 1\n",
    "        total_weight = weight_any + weight_pf\n",
    "        weight_any = weight_any / total_weight\n",
    "        weight_pf = weight_pf / total_weight\n",
    "\n",
    "    # Update the weights\n",
    "    update_weights(loss_any.detach(), loss_pf.detach())\n",
    "\n",
    "    # Combine the losses (you can adjust initial learning rates if needed)\n",
    "    combined_loss = weight_any * loss_any + weight_pf * loss_pf\n",
    "\n",
    "    print(f\"Combined loss: {combined_loss}\")\n",
    "    return combined_loss\n",
    "\n",
    "# Custom loss function for multitask learning using proportional loss weighting\n",
    "# This function computes the combined loss for multiple tasks\n",
    "# The loss for each task is computed using binary cross-entropy loss\n",
    "def multitask_loss_loss_proportional(preds, targets, epsilon=1e-8):\n",
    "    pred_any, pred_pf = preds\n",
    "    y_any, y_pf = targets\n",
    "\n",
    "    # Reshape targets\n",
    "    y_any = y_any.view(-1, 1)\n",
    "    y_pf = y_pf.view(-1, 1)\n",
    "\n",
    "    # Ensure predictions and targets are on the same device\n",
    "    pred_any, pred_pf = pred_any.to(y_any.device), pred_pf.to(y_pf.device)\n",
    "\n",
    "    loss_any = binary_cross_entropy(pred_any, y_any)\n",
    "    loss_pf = binary_cross_entropy(pred_pf, y_pf)\n",
    "\n",
    "    # Calculate weights\n",
    "    weight_any = 1 / (loss_any.detach().item() + epsilon)\n",
    "    weight_pf = 1 / (loss_pf.detach().item() + epsilon)\n",
    "\n",
    "    # Normalize weights\n",
    "    total_weight = weight_any + weight_pf\n",
    "    weight_any = weight_any / total_weight\n",
    "    weight_pf = weight_pf / total_weight\n",
    "\n",
    "    combined_loss = weight_any * loss_any + weight_pf * loss_pf\n",
    "\n",
    "    print(f\"Combined loss: {combined_loss}\")\n",
    "    return combined_loss\n",
    "\n",
    "# Custom splitter to define paramater groups for multitask learning\n",
    "# This function splits the model parameters into three groups:\n",
    "# 1. RNN shared parameters\n",
    "# 2. Predictions for 'any' task\n",
    "# 3. Predictions for 'pf' task\n",
    "def multitask_splitter(model):\n",
    "    if isinstance(model, MultiTaskRNN):\n",
    "        return [list(model.rnn.parameters()), list(model.pred_any.parameters()), list(model.pred_pf.parameters())]\n",
    "    elif isinstance(model, MultiTaskTransformer):\n",
    "        return [list(model.transformer.parameters()), list(model.pred_any.parameters()), list(model.pred_pf.parameters())]\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model: {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **3. RNN Training and Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters needed for Hyperparamter searches and model training\n",
    "samples = 60\n",
    "search_epoch = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **3.1 RNN Grid Search**\n",
    "\n",
    "#### Let's start by using this section to perform a grid search over our RNN and find the best hyperparameters for our model to train with. Here's how I've set up my hyperparameter optimization using Bayesian Optimization:\n",
    "\n",
    "---\n",
    "\n",
    "### **Hyperparameter Search with Bayesian Optimization**\n",
    "\n",
    "* I'm using `gp_minimize` from the `skopt` library to perform Bayesian Optimization. This allows for an efficient exploration of the hyperparameter space by balancing exploration and exploitation.\n",
    "* The search space (`rnn_param_space`) is defined using `Dimension` objects from `skopt.space`. The hyperparameters being optimized include:\n",
    "    * **`hidden_size`**: The size of the hidden layer in the LSTM, defined as an integer range.\n",
    "    * **`num_layers`**: The number of LSTM layers, also defined as an integer range.\n",
    "    * **`optimizer`**: A categorical choice between optimizers (`'SGD'`, `'Adam'`, and `'AdamW'`).\n",
    "    * **`loss_func`**: A categorical choice between custom multi-task loss functions.\n",
    "    * **`patience`**: The patience parameter for early stopping, defined as an integer range.\n",
    "    * **`lr`**: The learning rate, defined as a real-valued range.\n",
    "\n",
    "* To ensure robust evaluation, I'm using **k-fold cross-validation** (`k_folds = 5`) with the `KFold` class from `sklearn.model_selection`. This helps estimate the model's performance across different data splits.\n",
    "\n",
    "### **Objective Function**\n",
    "\n",
    "* The `objective` function is the core of the optimization process and is minimized by `gp_minimize`.\n",
    "* It takes hyperparameter values as input (using the `@use_named_args` decorator for clean parameter handling).\n",
    "* Inside the `objective` function:\n",
    "    * The data is split into training and validation sets for each fold.\n",
    "    * The `train_and_evaluate_rnn` function is called for each fold to train and evaluate the RNN model.\n",
    "    * The average validation loss across all folds is calculated.\n",
    "* The `objective` function returns the average validation loss, which `gp_minimize` tries to minimize.\n",
    "\n",
    "### **Training and Evaluation**\n",
    "\n",
    "#### `train_and_evaluate_rnn`\n",
    "\n",
    "* This function handles the training and evaluation of the RNN model for a given set of hyperparameters and data splits.\n",
    "* Key steps include:\n",
    "    1. **Extracting Hyperparameters**: The function extracts the hyperparameters passed to it.\n",
    "    2. **Data Splitting**: The data is split into training and validation sets for the current fold.\n",
    "    3. **Data Conversion**: The data is converted into PyTorch tensors using the `convert_to_tensor` function.\n",
    "    4. **DataLoaders Creation**: Training and validation datasets are wrapped into `DataLoaders` using the `MultiTaskDataset` class.\n",
    "    5. **Model Initialization**: The `MultiTaskRNN` model is instantiated with the given hyperparameters.\n",
    "    6. **Learner Setup**: A `Learner` object (from `fastai.learner`) is created for training, with custom loss functions and metrics.\n",
    "    7. **Early Stopping**: Early stopping is implemented to halt training if the validation loss does not improve for a specified number of epochs.\n",
    "    8. **Model Saving**: The best model (based on validation loss) is saved and reloaded for final evaluation.\n",
    "* The function returns the validation loss and F1 score for the current fold.\n",
    "\n",
    "### **Bayesian Optimization Execution**\n",
    "\n",
    "* The Bayesian Optimization process is executed using `gp_minimize`, which minimizes the `objective` function over the defined search space.\n",
    "* Key parameters:\n",
    "    * **`objective`**: The function to minimize (average validation loss).\n",
    "    * **`rnn_param_space`**: The hyperparameter search space.\n",
    "    * **`n_calls`**: The number of iterations for the optimization process.\n",
    "* After the optimization:\n",
    "    * The best hyperparameters are extracted and converted to their appropriate types (e.g., integers, floats, or categorical values).\n",
    "    * The best validation loss is also recorded.\n",
    "    \n",
    "### **Summary**\n",
    "\n",
    "This section efficiently searches for the best combination of hyperparameters for the RNN model using Bayesian Optimization. By leveraging k-fold cross-validation, the process ensures robust evaluation of each hyperparameter configuration. The use of early stopping further enhances the training process by preventing overfitting and saving computational resources. The result is an optimized RNN model with hyperparameters that minimize validation loss while maintaining strong performance across folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import gp_minimize\n",
    "from skopt.space import Dimension, Categorical, Integer, Real as Real_skopt\n",
    "from skopt.utils import use_named_args\n",
    "from itertools import product\n",
    "from fastai.data.all import *\n",
    "from fastai.learner import *\n",
    "from fastai.metrics import *\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Fixed parameters\n",
    "batch_size = 64\n",
    "\n",
    "# Number of folds for cross-validation\n",
    "k_folds = 5\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Function to train and evaluate the RNN model (remains mostly the same)\n",
    "def train_and_evaluate_rnn(params, train_idx, val_idx, shared_weight_decay = 0.003, task_weight_decay = 0.001, dropout=0.4):\n",
    "    # Extract parameters\n",
    "    # Conditional unpacking based on the number of parameters\n",
    "    if len(params) == 6:\n",
    "        hidden_size, num_layers, optimizer, multitask_loss_func, patience_param, lr = params\n",
    "    elif len(params) == 9:\n",
    "        hidden_size, num_layers, optimizer, multitask_loss_func, patience_param, lr, shared_weight_decay, task_weight_decay, dropout = params\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected number of parameters: {len(params)}\")\n",
    "\n",
    "    # Split the data into training and validation sets for the current fold\n",
    "    X_train_fold, X_val_fold = X_train[train_idx], X_train[val_idx]\n",
    "    y_train_any_fold, y_val_any_fold = y_train_any[train_idx], y_train_any[val_idx]\n",
    "    y_train_pf_fold, y_val_pf_fold = y_train_pf[train_idx], y_train_pf[val_idx]\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_tensor, X_val_tensor, y_train_any_tensor, y_train_pf_tensor, y_val_any_tensor, y_val_pf_tensor = convert_to_tensor(X_train_fold, X_val_fold, y_train_any_fold, y_train_pf_fold, y_val_any_fold, y_val_pf_fold)\n",
    "\n",
    "    # Create training and validation datasets\n",
    "    train_ds = MultiTaskDataset(X_train_tensor, y_train_any_tensor, y_train_pf_tensor)\n",
    "    val_ds = MultiTaskDataset(X_val_tensor, y_val_any_tensor, y_val_pf_tensor)\n",
    "\n",
    "    print(f\"Train Dset: {train_ds}\")\n",
    "    print(f\"Val Dset: {val_ds}\")\n",
    "    \n",
    "    # Update DataLoaders with the current batch size\n",
    "    dls = DataLoaders.from_dsets(train_ds, val_ds, bs=batch_size, device=device)\n",
    "\n",
    "    # Debugging: Print the first batch of the training DataLoader\n",
    "    # for batch in dls.train:\n",
    "    #     print(f\"Batch: {batch}\")\n",
    "    #     break\n",
    "    \n",
    "    # Instantiate the model with the current parameters\n",
    "    # Since we have already fit and transformed the data into a flattened 2D array, we can use the input size directly\n",
    "    model = MultiTaskRNN(input_size=X_train.shape[1], hidden_size=hidden_size, num_layers=num_layers, dropout=dropout)\n",
    "    model = model.to(device)  # Move model to GPU if available\n",
    "    \n",
    "    # Create Learner\n",
    "    rnn_learn = Learner(dls, model, loss_func=multitask_loss_func, opt_func=get_optimizer(optimizer, lr), splitter=multitask_splitter, metrics=[MultiTaskMetric(0), MultiTaskMetric(1), MultiTaskTotalMetric()])\n",
    "    # rnn_learn.add_cb(DebugCallback()) # uncomment for debugging needs\n",
    "\n",
    "    # Initialize the optimizer\n",
    "    rnn_learn.create_opt()\n",
    "\n",
    "    # Set weight decay for each parameter group\n",
    "    rnn_learn.opt.set_hypers(wd=[shared_weight_decay, task_weight_decay, task_weight_decay])\n",
    "\n",
    "    # Early Stopping Implementation for Grid Search \n",
    "    patience = patience_param  \n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "    epochs_no_improve = 0  \n",
    "\n",
    "    for epoch in range(search_epoch):\n",
    "        rnn_learn.fit(1, lr=lr)  \n",
    "\n",
    "        # Evaluate on validation set\n",
    "        val_loss = rnn_learn.validate()[0]  \n",
    "\n",
    "        print(f\"Epoch: {epoch + 1}, Validation Loss: {val_loss}\")\n",
    "\n",
    "        # Early stopping check within the 10-epoch limit\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "            epochs_no_improve = 0  \n",
    "            torch.save(rnn_learn.model.state_dict(), 'best_model.pth')  # Save the best model\n",
    "        else:\n",
    "            counter += 1\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        # Check for stopping condition: patience or consistent increase\n",
    "        if counter >= patience or epochs_no_improve >= 8:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "    # Load the best model\n",
    "    rnn_learn.model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "    # Evaluate on the validation set one last time\n",
    "    validation_loss = rnn_learn.validate()[0]\n",
    "\n",
    "    # Get the F1 score from the MultiTaskTotalMetric\n",
    "    f1_score = rnn_learn.metrics[2].metrics['f1_score']\n",
    "\n",
    "    # Clear CUDA cache to prevent memory issues after each fold\n",
    "    if device.type == 'cuda':\n",
    "      torch.cuda.empty_cache()\n",
    "\n",
    "    return validation_loss, f1_score\n",
    "\n",
    "def run_bayesian_optimization(objective, samples, rnn_param_space):\n",
    "    # Perform Bayesian Optimization for RNN with average validation loss objective\n",
    "    n_calls = samples  # Number of times to sample the objective function\n",
    "    result = gp_minimize(objective, rnn_param_space, n_calls=n_calls, random_state=42)\n",
    "\n",
    "    # Ensure proper types for the results\n",
    "    best_rnn_params = {}\n",
    "\n",
    "    # Extract the best parameters and convert them to the correct types\n",
    "    for param, value in zip(rnn_param_space, result.x):\n",
    "        if isinstance(param, Integer):\n",
    "            best_rnn_params[param.name] = int(value)\n",
    "        elif isinstance(param, Real_skopt):\n",
    "            best_rnn_params[param.name] = float(value)\n",
    "        elif isinstance(param, Categorical):\n",
    "            best_rnn_params[param.name] = value\n",
    "\n",
    "    # Print the best parameters and loss\n",
    "    print(\"Best RNN Parameters:\", best_rnn_params)\n",
    "    print(\"Best RNN Validation Loss:\", result.fun)\n",
    "\n",
    "    return best_rnn_params\n",
    "\n",
    "# Run the best RNN model training with the best parameters found from Bayesian Optimization\n",
    "def run_best_rnn_model_training(hidden_size, num_layers, optimizer, multitask_loss_func, patience, lr, shared_weight_decay, task_weight_decay, dropout, epochs=100):\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_tensor, X_val_tensor, y_train_any_tensor, y_train_pf_tensor, y_val_any_tensor, y_val_pf_tensor = convert_to_tensor(X_train, X_val, y_train_any, y_train_pf, y_val_any, y_val_pf)\n",
    "\n",
    "    # Create training and validation datasets\n",
    "    train_ds = MultiTaskDataset(X_train_tensor, y_train_any_tensor, y_train_pf_tensor)\n",
    "    val_ds = MultiTaskDataset(X_val_tensor, y_val_any_tensor, y_val_pf_tensor)\n",
    "\n",
    "    # Update DataLoaders with the best batch size\n",
    "    dls = DataLoaders.from_dsets(train_ds, val_ds, bs=batch_size)\n",
    "\n",
    "    # Instantiate the model with the best parameters\n",
    "    best_rnn_model = MultiTaskRNN(input_size=X_train.shape[1], hidden_size=hidden_size, num_layers=num_layers, dropout=dropout)\n",
    "\n",
    "    # Create Learner\n",
    "    best_rnn_learn = Learner(dls, best_rnn_model, loss_func=multitask_loss_func, opt_func=get_optimizer(optimizer, lr), splitter=multitask_splitter, metrics=[MultiTaskMetric(0), MultiTaskMetric(1), MultiTaskTotalMetric()])\n",
    "    # best_rnn_learn.add_cb(DebugCallback()) # uncomment for debugging needs\n",
    "\n",
    "    # Initialize the optimizer\n",
    "    best_rnn_learn.create_opt()\n",
    "\n",
    "    # Set weight decay for each parameter group\n",
    "    best_rnn_learn.opt.set_hypers(wd=[shared_weight_decay, task_weight_decay, task_weight_decay])\n",
    "\n",
    "    # Early Stopping implementation for RNN Grid Search\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "    for epoch in range(epochs): \n",
    "        best_rnn_learn.fit(1, lr=lr)\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        val_loss = best_rnn_learn.validate()[0]\n",
    "\n",
    "        print(f\"Epoch: {epoch + 1}, Validation Loss: {val_loss}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "            torch.save(best_rnn_learn.model.state_dict(), 'best_rnn_model.pth')  # Save the best model\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(\"Early stopping triggered in final training!\")\n",
    "                break\n",
    "\n",
    "    # Load the best model\n",
    "    best_rnn_learn.model.load_state_dict(torch.load('best_rnn_model.pth'))\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    validation_loss = best_rnn_learn.validate()[0]\n",
    "    combined_metrics = best_rnn_learn.validate()[3]\n",
    "\n",
    "    print(f\"Validation Loss: {validation_loss}\")\n",
    "    print(\"-------------------------------------------------\")\n",
    "    print(f\"Combined Metrics: {combined_metrics}\")\n",
    "\n",
    "    return best_rnn_learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **3.2 RNN Grid Search With Average Validation Loss All Params**\n",
    "\n",
    "#### Let's do another grid search with a Combined Score Objective to see if we can find a better model. Here's how I've set up my hyperparameter optimization using Bayesian Optimization:\n",
    "\n",
    "#### This code implements **Bayesian Optimization** to tune hyperparameters for an RNN model by optimizing a **average loss validation function**. \n",
    "\n",
    "### **Hyperparameter Search Space**\n",
    "\n",
    "The search space (`rnn_param_space`) includes:\n",
    "- **`hidden_size`**: The size of the LSTM hidden layer (integer range: 256–1024).\n",
    "- **`num_layers`**: The number of LSTM layers (integer range: 2–8).\n",
    "- **`optimizer`**: A categorical choice between `'SGD'`, `'Adam'`, and `'AdamW'`.\n",
    "- **`loss_func`**: A categorical choice between custom multi-task loss functions.\n",
    "- **`patience`**: The patience parameter for early stopping (integer range: 3–15).\n",
    "- **`lr`**: The learning rate (real-valued range: 0.00001–0.006).\n",
    "- **`shared_weight_decay`**: Weight decay for shared layers (real-valued range: 0.0001–0.01).\n",
    "- **`task_weight_decay`**: Weight decay for task-specific layers (real-valued range: 0.0001–0.01).\n",
    "- **`dropout`**: Dropout rate (real-valued range: 0.1–0.5).\n",
    "\n",
    "### **Objective Function**\n",
    "\n",
    "The `objective_avg_loss` function:\n",
    "- Performs **k-fold cross-validation** (using `kf.split`) to evaluate the model's performance for each hyperparameter configuration.\n",
    "- Calls `train_and_evaluate_rnn` for each fold to train and evaluate the RNN model.\n",
    "- Computes the **average validation loss** across all folds, which is returned as the objective to minimize.\n",
    "\n",
    "### **Bayesian Optimization Execution**\n",
    "\n",
    "- The `run_bayesian_optimization` function is used to minimize the `objective_avg_loss` function over the defined search space.\n",
    "- The best hyperparameters (`best_rnn_params`) are extracted after the optimization process.\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "This section performs a comprehensive search for the optimal hyperparameters for the RNN model. By including additional parameters like `shared_weight_decay`, `task_weight_decay`, and `dropout`, the search aims to fine-tune both regularization and model architecture. The use of Bayesian Optimization and k-fold cross-validation ensures robust evaluation and efficient exploration of the hyperparameter space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authors' recommended parameters\n",
    "# dropout = 0.4\n",
    "# batch_size = 64\n",
    "# shared_weight_decay = 0.003\n",
    "# task_weight_decay = 0.001\n",
    "\n",
    "# Define the hyperparameter search space for Bayesian Optimization\n",
    "rnn_param_space_avg_loss = [\n",
    "    Integer(name='hidden_size', low=128, high=512),\n",
    "    Integer(name='num_layers', low=2, high=8),\n",
    "    Categorical(name='optimizer', categories=['SGD', 'Adam', 'AdamW']),\n",
    "    Categorical(name='loss_func', categories=[multitask_loss, multitask_loss_loss_proportional, multitask_loss_auto_weighted_bce, multitask_loss_dynamic_weighted]),\n",
    "    Integer(name='patience', low=5, high=15),\n",
    "    Real_skopt(name='lr', low=0.00001, high=0.006),\n",
    "    Real_skopt(name='shared_weight_decay', low=0.0001, high=0.01),\n",
    "    Real_skopt(name='task_weight_decay', low=0.0001, high=0.01),\n",
    "    Real_skopt(name='dropout', low=0.1, high=0.5)\n",
    "]\n",
    "\n",
    "# Objective function for Bayesian Optimization with average validation loss\n",
    "@use_named_args(rnn_param_space_avg_loss)\n",
    "def objective_avg_loss(hidden_size, num_layers, optimizer, loss_func, patience, lr, shared_weight_decay, task_weight_decay, dropout):\n",
    "    fold_losses = []\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    for train_idx, val_idx in kf.split(X_train):\n",
    "        # Train and evaluate the model on the current fold\n",
    "        validation_loss, _ = train_and_evaluate_rnn((int(hidden_size), int(num_layers), optimizer, loss_func, int(patience), lr, shared_weight_decay, task_weight_decay, dropout), train_idx, val_idx)\n",
    "\n",
    "        fold_losses.append(validation_loss)\n",
    "\n",
    "    # Calculate the average validation loss across all folds\n",
    "    avg_validation_loss = sum(fold_losses) / len(fold_losses)\n",
    "\n",
    "    print(f\"Params: {(hidden_size, num_layers, optimizer, loss_func.__name__, patience, lr)}, Avg Validation Loss: {avg_validation_loss}\")\n",
    "\n",
    "    return avg_validation_loss\n",
    "\n",
    "# Perform Bayesian Optimization for RNN with baseline objective\n",
    "best_rnn_params_avg_loss = run_bayesian_optimization(objective_avg_loss, samples, rnn_param_space_avg_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **3.3 Train RNN with Best Parameters for Average Validation Loss Objective**\n",
    "\n",
    "#### Now we will train the RNN with the best parameters we found in our grid search for the Average Validation Loss Objective and saved in **best_rnn_params**\n",
    "#### After we will print out a series of important metrics and visualizations to verify our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.data.all import *\n",
    "from fastai.learner import *\n",
    "from fastai.metrics import *\n",
    "\n",
    "# To run with best parameters from previous test\n",
    "# Best RNN Parameters after first run of Bayesian Optimization with 60 samples and 20 epochs\n",
    "# Best RNN Parameters: {'hidden_size': 128, 'num_layers': 2, 'optimizer': np.str_('AdamW'), 'loss_func': <function multitask_loss_loss_proportional at 0x0000024B235A9120>, 'patience': 15, 'lr': 0.006, 'shared_weight_decay': 0.01, 'task_weight_decay': 0.0001, 'dropout': 0.5}\n",
    "# Best RNN Validation Loss: 0.17900913655757905\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "optimizer = 'AdamW'\n",
    "multitask_loss_func = multitask_loss_loss_proportional\n",
    "lr = 0.006\n",
    "patience = 15\n",
    "dropout = 0.5\n",
    "shared_weight_decay = 0.01\n",
    "task_weight_decay = 0.0001\n",
    "\n",
    "# Use Authors' recommended parameters for the baseline evaluation\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "\n",
    "# print(\"Best RNN Parameters:\", best_rnn_params_avg_loss)\n",
    "\n",
    "# # Extract the best parameters\n",
    "# dropout = best_rnn_params_avg_loss['dropout']\n",
    "# hidden_size = int(best_rnn_params_avg_loss['hidden_size'])\n",
    "# lr = best_rnn_params_avg_loss['lr']\n",
    "# num_layers = int(best_rnn_params_avg_loss['num_layers'])\n",
    "# multitask_loss_func = best_rnn_params_avg_loss['loss_func']\n",
    "# optimizer = best_rnn_params_avg_loss['optimizer']\n",
    "# patience = int(best_rnn_params_avg_loss['patience'])\n",
    "# shared_weight_decay = best_rnn_params_avg_loss['shared_weight_decay']\n",
    "# task_weight_decay = best_rnn_params_avg_loss['task_weight_decay']\n",
    "\n",
    "# Run the best RNN model training with the average loss objective parameters\n",
    "best_rnn_learn_avg_loss = run_best_rnn_model_training(hidden_size, num_layers, optimizer, multitask_loss_func, patience, lr, shared_weight_decay, task_weight_decay, dropout, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **3.4 Visualize Our Results for Average Validation Loss Objective**\n",
    "\n",
    "#### Let us visualize the results of our model training and evaluation for our Average Validation Loss Objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate and visualize the model performance and results\n",
    "evaluate_and_visualize(learn=best_rnn_learn_avg_loss, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **3.5 RNN Grid Search With Combined Score Objective**\n",
    "\n",
    "#### Let's do another grid search with a Combined Score Objective to see if we can find a better model. Here's how I've set up my hyperparameter optimization using Bayesian Optimization:\n",
    "\n",
    "#### This code implements **Bayesian Optimization** to tune hyperparameters for an RNN model by optimizing a **combined objective function** that balances **F1 score** (to maximize) and **validation loss** (to minimize). \n",
    "\n",
    "### **Hyperparameter Search Space**\n",
    "\n",
    "The search space (`rnn_param_space`) includes:\n",
    "- **`hidden_size`**: The size of the LSTM hidden layer (integer range: 256–1024).\n",
    "- **`num_layers`**: The number of LSTM layers (integer range: 2–8).\n",
    "- **`optimizer`**: A categorical choice between `'SGD'`, `'Adam'`, and `'AdamW'`.\n",
    "- **`loss_func`**: A categorical choice between custom multi-task loss functions.\n",
    "- **`patience`**: The patience parameter for early stopping (integer range: 3–15).\n",
    "- **`lr`**: The learning rate (real-valued range: 0.00001–0.006).\n",
    "- **`shared_weight_decay`**: Weight decay for shared layers (real-valued range: 0.0001–0.01).\n",
    "- **`task_weight_decay`**: Weight decay for task-specific layers (real-valued range: 0.0001–0.01).\n",
    "- **`dropout`**: Dropout rate (real-valued range: 0.1–0.5).\n",
    "\n",
    "### **Objective Function**\n",
    "\n",
    "The `objective_combined` function:\n",
    "- Performs **k-fold cross-validation** to evaluate the model's performance for each hyperparameter configuration.\n",
    "- Computes the **average F1 score** (to maximize) and **average validation loss** (to minimize) across all folds.\n",
    "- Combines these metrics into a single objective using weighted contributions:\n",
    "  - **F1 score**: Weighted higher (e.g., `0.8`) to prioritize maximizing performance.\n",
    "  - **Validation loss**: Weighted lower (e.g., `0.2`) to ensure regularization.\n",
    "\n",
    "### **Bayesian Optimization Execution**\n",
    "\n",
    "- The `gp_minimize` function minimizes the `objective_combined` function over the defined search space.\n",
    "- The best hyperparameters (`best_rnn_params`) are extracted and converted to their appropriate types (e.g., integers, floats, or categorical values).\n",
    "- The best combined objective value is recorded.\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "This section optimizes the RNN model's hyperparameters by balancing F1 score and validation loss using a combined objective. The use of Bayesian Optimization ensures efficient exploration of the hyperparameter space, while k-fold cross-validation provides robust evaluation. The result is a set of hyperparameters that maximize F1 score while maintaining low validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authors' recommended parameters\n",
    "# dropout = 0.4\n",
    "# batch_size = 64\n",
    "# shared_weight_decay = 0.003\n",
    "# task_weight_decay = 0.001\n",
    "\n",
    "# Define the hyperparameter search space for Bayesian Optimization\n",
    "rnn_param_space_combined_score = [\n",
    "    Integer(name='hidden_size', low=128, high=512),\n",
    "    Integer(name='num_layers', low=2, high=8),\n",
    "    Categorical(name='optimizer', categories=['SGD', 'Adam', 'AdamW']),\n",
    "    Categorical(name='loss_func', categories=[multitask_loss, multitask_loss_loss_proportional, multitask_loss_auto_weighted_bce, multitask_loss_dynamic_weighted]),\n",
    "    Integer(name='patience', low=5, high=15),\n",
    "    Real_skopt(name='lr', low=0.00001, high=0.006),\n",
    "    Real_skopt(name='shared_weight_decay', low=0.0001, high=0.01),\n",
    "    Real_skopt(name='task_weight_decay', low=0.0001, high=0.01),\n",
    "    Real_skopt(name='dropout', low=0.1, high=0.5)\n",
    "]\n",
    "\n",
    "# Objective function for Bayesian Optimization with combined objective (f1 score and validation loss)\n",
    "@use_named_args(rnn_param_space_combined_score)\n",
    "def objective_combined(hidden_size, num_layers, optimizer, loss_func, patience, lr, shared_weight_decay, task_weight_decay, dropout):\n",
    "    fold_f1_scores = []\n",
    "    fold_validation_losses = []\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    for train_idx, val_idx in kf.split(X_train):\n",
    "        # Train and evaluate the model on the current fold\n",
    "        validation_loss, f1_score = train_and_evaluate_rnn((int(hidden_size), int(num_layers), optimizer, loss_func, int(patience), lr, shared_weight_decay, task_weight_decay, dropout), train_idx, val_idx)\n",
    "\n",
    "        fold_f1_scores.append(f1_score)\n",
    "        fold_validation_losses.append(validation_loss)\n",
    "\n",
    "    # Calculate the average F1 score and validation loss across all folds\n",
    "    avg_f1_score = sum(fold_f1_scores) / len(fold_f1_scores)\n",
    "    avg_validation_loss = sum(fold_validation_losses) / len(fold_validation_losses)\n",
    "\n",
    "    # Combine the two metrics into a single objective (adjust weights as needed)\n",
    "    # Higher weight for F1 score (maximize) and lower weight for validation loss (minimize)\n",
    "    weight_f1 = 0.8\n",
    "    weight_loss = 0.2\n",
    "    combined_objective = -weight_f1 * avg_f1_score + weight_loss * avg_validation_loss\n",
    "\n",
    "    print(f\"Params: {(hidden_size, num_layers, optimizer, loss_func.__name__, patience, lr)}\\n\"f\"Avg F1 Score: {avg_f1_score} | Avg Validation Loss: {avg_validation_loss} | Combined Objective: {combined_objective}\")\n",
    "\n",
    "    return combined_objective\n",
    "\n",
    "# Perform Bayesian Optimization for RNN with average validation loss objective\n",
    "n_calls = samples  # Number of times to sample the objective function\n",
    "result = gp_minimize(objective_combined, rnn_param_space_combined_score, n_calls=n_calls, random_state=42)\n",
    "\n",
    "# Ensure proper types for the results\n",
    "best_rnn_params_combined_score = run_bayesian_optimization(objective_combined, samples, rnn_param_space_combined_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **3.6 Train RNN with Best Parameters for Combined Score Objective**\n",
    "\n",
    "#### Now we will train the RNN with the best parameters we found in our grid search for the Combiend Score Objective and saved in **best_rnn_params**\n",
    "#### After we will print out a series of important metrics and visualizations to verify our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.data.all import *\n",
    "from fastai.learner import *\n",
    "from fastai.metrics import *\n",
    "\n",
    "# To run with previous test best parameters\n",
    "# Best RNN Parameters after first run of Bayesian Optimization with 60 samples and 20 epochs\n",
    "# Best RNN Parameters: {'hidden_size': 200, 'num_layers': 2, 'optimizer': np.str_('AdamW'), 'loss_func': <function multitask_loss_dynamic_weighted at 0x00000182204FFBA0>, 'patience': 13, 'lr': 0.002786683553419411, 'shared_weight_decay': 0.0032307513112844535, 'task_weight_decay': 0.00046888866964757425, 'dropout': 0.4314006713537418}\n",
    "# Best RNN Validation Loss: -0.46442934273667535\n",
    "hidden_size = 200\n",
    "num_layers = 2\n",
    "optimizer = 'AdamW'\n",
    "multitask_loss_func = multitask_loss_dynamic_weighted\n",
    "lr = 0.002786683553419411\n",
    "patience = 13\n",
    "\n",
    "# Extract the best parameters\n",
    "epochs = 100\n",
    "dropout = 0.4314006713537418\n",
    "batch_size = 64\n",
    "shared_weight_decay = 0.0032307513112844535\n",
    "task_weight_decay = 0.00046888866964757425\n",
    "\n",
    "# print(\"Best RNN Parameters:\", best_rnn_params_combined_score)\n",
    "\n",
    "# # Extract parameters\n",
    "# hidden_size = int(best_rnn_params_combined_score['hidden_size'])\n",
    "# num_layers = int(best_rnn_params_combined_score['num_layers'])\n",
    "# optimizer = best_rnn_params_combined_score['optimizer']\n",
    "# multitask_loss_func = best_rnn_params_combined_score['loss_func']\n",
    "# patience = int(best_rnn_params_combined_score['patience'])\n",
    "# lr = best_rnn_params_combined_score['lr']\n",
    "\n",
    "# Run the best RNN model training with the combined score objective parameters\n",
    "best_rnn_learn_combined_score = run_best_rnn_model_training(hidden_size, num_layers, optimizer, multitask_loss_func, patience, lr, shared_weight_decay, task_weight_decay, dropout, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **3.7 Visualize Our Results for Combined Score**\n",
    "#### Let us visualize the results of our model training and evaluation for our Combined Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate and visualize the model performance and results\n",
    "evaluate_and_visualize(learn=best_rnn_learn_combined_score, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **4. Transformer Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters needed for Hyperparamter searches and model training\n",
    "samples = 60\n",
    "search_epoch = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **4.1 Transformer Grid Search**\n",
    "\n",
    "#### Let's start by using this section to perform a grid search over our Transformer model and find the best hyperparameters for our model to train with. Here's how I've set up my hyperparameter optimization using Bayesian Optimization:\n",
    "\n",
    "### Hyperparameter Search with Bayesian Optimization\n",
    "\n",
    "* I'm using `gp_minimize` from the `skopt` library to perform Bayesian Optimization. This helps me efficiently search the hyperparameter space.\n",
    "* I've defined the search space (`transformer_param_space`) using `Dimension` objects from `skopt.space`. This space includes:\n",
    "    * `hidden_size`: An integer range for the Transformer hidden size.\n",
    "    * `num_layers`: An integer range for the number of Transformer layers.\n",
    "    * `num_heads`: A categorical choice of valid number of attention heads, ensuring it's a divisor of `input_size`.\n",
    "    * `optimizer`: A categorical choice between 'SGD', 'Adam', and 'AdamW' optimizers.\n",
    "    * `loss_func`: A categorical choice between my custom multi-task loss functions.\n",
    "    * `patience`: An integer range for the early stopping patience parameter.\n",
    "    * `lr`: A real-valued range for the learning rate.\n",
    "* I'm employing k-fold cross-validation (with `k_folds = 5` and `KFold` from `sklearn.model_selection`) to get a robust estimate of the model's performance for each hyperparameter configuration.\n",
    "\n",
    "### Objective Function\n",
    "\n",
    "* The `objective` function is what `gp_minimize` optimizes.\n",
    "* It takes hyperparameter values as input (thanks to the `@use_named_args` decorator).\n",
    "* Inside `objective`:\n",
    "    * I perform the k-fold cross-validation.\n",
    "    * For each fold, I train and evaluate my Transformer model using the `train_and_evaluate_transformer` function.\n",
    "    * I calculate the average validation loss across all folds.\n",
    "* The `objective` function returns this average validation loss, which `gp_minimize` tries to minimize.\n",
    "\n",
    "### Training and Evaluation\n",
    "\n",
    "`train_and_evaluate_transformer`\n",
    "\n",
    "* This function trains and evaluates my Transformer model for a given set of hyperparameters and data folds.\n",
    "* It does the following:\n",
    "    * Extracts hyperparameters.\n",
    "    * Splits the data into training and validation sets for the current fold.\n",
    "    * Converts the data to PyTorch tensors using `convert_to_tensor`.\n",
    "    * Creates training and validation `DataLoaders` (using `MultiTaskDataset`).\n",
    "    * Instantiates the `MultiTaskTransformer` model with the given hyperparameters.\n",
    "    * Creates a `Learner` (from `fastai.learner`) for training.\n",
    "    * Implements early stopping based on validation loss.\n",
    "    * Finally, it returns the validation loss.\n",
    "\n",
    "### Bayesian Optimization Execution\n",
    "\n",
    "* I run the Bayesian Optimization using `gp_minimize`, specifying the `objective` function and the search space.\n",
    "* The `n_calls` parameter controls how many iterations the optimization runs for.\n",
    "* After the optimization, I extract the best hyperparameters and the corresponding best validation loss.\n",
    "\n",
    "In essence, this code efficiently searches for the best combination of hyperparameters for my Transformer model by using Bayesian Optimization and k-fold cross-validation to minimize the validation loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import gp_minimize\n",
    "from skopt.space import Dimension, Categorical, Integer, Real as Real_skopt\n",
    "from skopt.utils import use_named_args\n",
    "from itertools import product\n",
    "from fastai.data.all import *\n",
    "from fastai.learner import *\n",
    "from fastai.metrics import *\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Ensure num_heads is a divisor of input_size and within the range we want to search ie. 4-16\n",
    "# If heads is not a divisor of input_size, the model will throw an error\n",
    "input_size = X_train.shape[1]\n",
    "valid_num_heads = [h for h in range(4, 17) if input_size % h == 0]\n",
    "\n",
    "# Number of folds for cross-validation\n",
    "k_folds = 5\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Function to train and evaluate the Transformer model (remains mostly the same)\n",
    "def train_and_evaluate_transformer(params, train_idx, val_idx, shared_weight_decay = 0.003, task_weight_decay = 0.001, dropout=0.4):\n",
    "    # Extract parameters\n",
    "    # Conditional unpacking based on the number of parameters\n",
    "    if len(params) == 7:\n",
    "        hidden_size, num_layers, num_heads, optimizer, multitask_loss_func, patience_param, lr = params\n",
    "    elif len(params) == 10:\n",
    "        hidden_size, num_layers, num_heads, optimizer, multitask_loss_func, patience_param, lr, shared_weight_decay, task_weight_decay, dropout = params\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected number of parameters: {len(params)}\")\n",
    "\n",
    "    # Split the data into training and validation sets for the current fold\n",
    "    X_train_fold, X_val_fold = X_train[train_idx], X_train[val_idx]\n",
    "    y_train_any_fold, y_val_any_fold = y_train_any[train_idx], y_train_any[val_idx]\n",
    "    y_train_pf_fold, y_val_pf_fold = y_train_pf[train_idx], y_train_pf[val_idx]\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_tensor, X_val_tensor, y_train_any_tensor, y_train_pf_tensor, y_val_any_tensor, y_val_pf_tensor = convert_to_tensor(X_train_fold, X_val_fold, y_train_any_fold, y_train_pf_fold, y_val_any_fold, y_val_pf_fold)\n",
    "\n",
    "    # Create training and validation datasets\n",
    "    train_ds = MultiTaskDataset(X_train_tensor, y_train_any_tensor, y_train_pf_tensor)\n",
    "    val_ds = MultiTaskDataset(X_val_tensor, y_val_any_tensor, y_val_pf_tensor)\n",
    "\n",
    "    print(f\"Train Dset: {train_ds}\")\n",
    "    print(f\"Val Dset: {val_ds}\")\n",
    "    \n",
    "    # Update DataLoaders with the current batch size\n",
    "    dls = DataLoaders.from_dsets(train_ds, val_ds, bs=batch_size, device=device)\n",
    "\n",
    "    # Debugging: Print the first batch of the training DataLoader\n",
    "    # for batch in dls.train:\n",
    "    #     print(f\"Batch: {batch}\")\n",
    "    #     break\n",
    "    \n",
    "    # Instantiate the Transformer model with the current parameters\n",
    "    model = MultiTaskTransformer(input_size=X_train.shape[1], hidden_size=hidden_size, num_layers=num_layers, num_heads=num_heads, dropout=dropout)\n",
    "    model = model.to(device)  # Move model to GPU if available\n",
    "    \n",
    "    # Create Learner\n",
    "    transformer_learn = Learner(dls, model, loss_func=multitask_loss_func, opt_func=get_optimizer(optimizer, lr), splitter=multitask_splitter,  metrics=[MultiTaskMetric(0), MultiTaskMetric(1), MultiTaskTotalMetric()])\n",
    "    # learn.add_cb(DebugCallback()) # uncomment for debugging needs\n",
    "\n",
    "    # Initialize the optimizer\n",
    "    transformer_learn.create_opt()\n",
    "\n",
    "    # Set weight decay for each parameter group\n",
    "    transformer_learn.opt.set_hypers(wd=[shared_weight_decay, task_weight_decay, task_weight_decay])\n",
    "    \n",
    "    # Early Stopping implementation for Transformer Grid Search\n",
    "    patience = patience_param\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "    for epoch in range(search_epoch):\n",
    "        transformer_learn.fit(1, lr=lr)\n",
    "\n",
    "        val_loss = transformer_learn.validate()[0]  # Get validation loss\n",
    "\n",
    "        print(f\"Epoch: {epoch + 1}, Validation Loss: {val_loss}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "            best_model_state = transformer_learn.model.state_dict()\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(\"Early stopping triggered!\")\n",
    "                break\n",
    "\n",
    "    transformer_learn.model.load_state_dict(best_model_state)  # Load the best model weights\n",
    "\n",
    "    # Evaluate on the validation set\n",
    "    validation_loss = transformer_learn.validate()[0]  \n",
    "\n",
    "    # Get the F1 score from the MultiTaskTotalMetric\n",
    "    f1_score = transformer_learn.metrics[2].metrics['f1_score']\n",
    "\n",
    "    # Clear CUDA cache to prevent memory issues after each fold\n",
    "    if device.type == 'cuda':\n",
    "      torch.cuda.empty_cache()\n",
    "\n",
    "    return validation_loss, f1_score\n",
    "\n",
    "def run_bayesian_optimization(objective, samples, transformer_param_space):\n",
    "    # Perform Bayesian Optimization for Transformer\n",
    "    n_calls = samples  # Number of times to sample the objective function\n",
    "    result = gp_minimize(objective, transformer_param_space, n_calls=n_calls, random_state=42)\n",
    "\n",
    "    # Ensure proper types for the results\n",
    "    best_transformer_params = {}\n",
    "\n",
    "    # Extract the best parameters and convert them to the correct types\n",
    "    for param, value in zip(transformer_param_space, result.x):\n",
    "        if isinstance(param, Integer):\n",
    "            best_transformer_params[param.name] = int(value)\n",
    "        elif isinstance(param, Real_skopt):\n",
    "            best_transformer_params[param.name] = float(value)\n",
    "        elif isinstance(param, Categorical):\n",
    "            best_transformer_params[param.name] = value\n",
    "\n",
    "    # Print the best parameters and loss\n",
    "    print(\"Best Transformer Parameters:\", best_transformer_params)\n",
    "    print(\"Best Transformer Validation Loss:\", result.fun)\n",
    "\n",
    "    return best_transformer_params\n",
    "\n",
    "# Run the best Transformer model training with the best parameters found from Bayesian Optimization\n",
    "def run_best_transformer_model_training(hidden_size, num_layers, num_heads, optimizer, multitask_loss_func, patience, lr, shared_weight_decay, task_weight_decay, dropout, epochs=100):\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_tensor, X_val_tensor, y_train_any_tensor, y_train_pf_tensor, y_val_any_tensor, y_val_pf_tensor = convert_to_tensor(X_train, X_val, y_train_any, y_train_pf, y_val_any, y_val_pf)\n",
    "\n",
    "    # Create training and validation datasets\n",
    "    train_ds = MultiTaskDataset(X_train_tensor, y_train_any_tensor, y_train_pf_tensor)\n",
    "    val_ds = MultiTaskDataset(X_val_tensor, y_val_any_tensor, y_val_pf_tensor)\n",
    "\n",
    "    # Update DataLoaders with the best batch size\n",
    "    dls = DataLoaders.from_dsets(train_ds, val_ds, bs=batch_size)\n",
    "\n",
    "    # Instantiate the Transformer model with the best parameters\n",
    "    best_transformer_model = MultiTaskTransformer(input_size=X_train.shape[1], hidden_size=hidden_size, num_layers=num_layers, num_heads=num_heads, dropout=dropout)\n",
    "\n",
    "    # Create Learner\n",
    "    best_transformer_learn = Learner(dls, best_transformer_model, loss_func=multitask_loss_func, opt_func=get_optimizer(optimizer, lr), splitter=multitask_splitter, metrics=[MultiTaskMetric(0), MultiTaskMetric(1), MultiTaskTotalMetric()])\n",
    "    # best_transformer_learn.add_cb(DebugCallback()) # uncomment for debugging needs\n",
    "\n",
    "    # Initialize the optimizer\n",
    "    best_transformer_learn.create_opt()\n",
    "\n",
    "    # Set weight decay for each parameter group\n",
    "    best_transformer_learn.opt.set_hypers(wd=[shared_weight_decay, task_weight_decay, task_weight_decay])\n",
    "\n",
    "    # Early Stopping implementation for RNN Grid Search\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "    for epoch in range(epochs): \n",
    "        best_transformer_learn.fit(1, lr=lr)\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        val_loss = best_transformer_learn.validate()[0]\n",
    "\n",
    "        print(f\"Epoch: {epoch + 1}, Validation Loss: {val_loss}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "            torch.save(best_transformer_learn.model.state_dict(), 'best_rnn_model.pth')  # Save the best model\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(\"Early stopping triggered in final training!\")\n",
    "                break\n",
    "\n",
    "    # Load the best model\n",
    "    best_transformer_learn.model.load_state_dict(torch.load('best_rnn_model.pth'))\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    combined_score_loss = best_transformer_learn.validate()[0]\n",
    "    combined_metrics = best_transformer_learn.validate()[3]\n",
    "\n",
    "    print(f\"Validation Loss: {combined_score_loss}\")\n",
    "    print(\"-------------------------------------------------\")\n",
    "    print(f\"Combined Metrics: {combined_metrics}\")\n",
    "\n",
    "    return best_transformer_learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **4.2 Hyperparameter Search with Bayesian Optimization for Average Loss Objective**\n",
    "\n",
    "#### Let's do grid search with a Average Loss Objective to see if we can find a better model. Here's how I've set up my hyperparameter optimization using Bayesian Optimization:\n",
    "\n",
    "#### This code implements **Bayesian Optimization** to tune hyperparameters for a Transformer model by optimizing a **average loss validation function**.\n",
    "\n",
    "### **Hyperparameter Search Space**\n",
    "\n",
    "The search space (`transformer_param_space`) includes:\n",
    "- **`hidden_size`**: The size of the Transformer hidden layer (integer range: 256–1024).\n",
    "- **`num_layers`**: The number of Transformer layers (integer range: 2–8).\n",
    "- **`num_heads`**: The number of Transformer layers (integer range: 2–16).\n",
    "- **`optimizer`**: A categorical choice between `'SGD'`, `'Adam'`, and `'AdamW'`.\n",
    "- **`loss_func`**: A categorical choice between custom multi-task loss functions.\n",
    "- **`patience`**: The patience parameter for early stopping (integer range: 3–15).\n",
    "- **`lr`**: The learning rate (real-valued range: 0.00001–0.006).\n",
    "- **`shared_weight_decay`**: Weight decay for shared layers (real-valued range: 0.0001–0.01).\n",
    "- **`task_weight_decay`**: Weight decay for task-specific layers (real-valued range: 0.0001–0.01).\n",
    "- **`dropout`**: Dropout rate (real-valued range: 0.1–0.5).\n",
    "\n",
    "### **Objective Function**\n",
    "\n",
    "The `objective_avg_loss` function:\n",
    "- Performs **k-fold cross-validation** to evaluate the model's performance for each hyperparameter configuration.\n",
    "- Calls `train_and_evaluate_rnn` for each fold to train and evaluate the Transformer model.\n",
    "- Computes the **average validation loss** across all folds, which is returned as the objective to minimize.\n",
    "\n",
    "### **Bayesian Optimization Execution**\n",
    "\n",
    "- The `run_bayesian_optimization` function is used to minimize the `objective_avg_loss` function over the defined search space.\n",
    "- The best hyperparameters (`best_transformer_params`) are extracted after the optimization process.\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "This section performs a comprehensive search for the optimal hyperparameters for the Transformer model. By including additional parameters like `shared_weight_decay`, `task_weight_decay`, and `dropout`, the search aims to fine-tune both regularization and model architecture. The use of Bayesian Optimization and k-fold cross-validation ensures robust evaluation and efficient exploration of the hyperparameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authors' recommended parameters\n",
    "# dropout = 0.4\n",
    "# batch_size = 64\n",
    "# shared_weight_decay = 0.003\n",
    "# task_weight_decay = 0.001\n",
    "\n",
    "# Define the hyperparameter search space for Bayesian Optimization\n",
    "transformer_param_space_avg_loss = [\n",
    "    Integer(name='hidden_size', low=256, high=1024),\n",
    "    Integer(name='num_layers', low=2, high=8),\n",
    "    Categorical(name='num_heads', categories=valid_num_heads),  # Use only valid num_heads\n",
    "    Categorical(name='optimizer', categories=['SGD', 'Adam', 'AdamW']),\n",
    "    Categorical(name='loss_func', categories=[multitask_loss, multitask_loss_loss_proportional, multitask_loss_auto_weighted_bce, multitask_loss_dynamic_weighted]),\n",
    "    Integer(name='patience', low=5, high=15),\n",
    "    Real_skopt(name='lr', low=0.00001, high=0.006),\n",
    "    Real_skopt(name='shared_weight_decay', low=0.0001, high=0.01),\n",
    "    Real_skopt(name='task_weight_decay', low=0.0001, high=0.01),\n",
    "    Real_skopt(name='dropout', low=0.1, high=0.5)\n",
    "]\n",
    "\n",
    "# Objective function for Bayesian Optimization with average validation loss\n",
    "@use_named_args(transformer_param_space_avg_loss)\n",
    "def objective_avg_loss(hidden_size, num_layers, num_heads, optimizer, loss_func, patience, lr, shared_weight_decay, task_weight_decay, dropout):\n",
    "    fold_losses = []\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    for train_idx, val_idx in kf.split(X_train):\n",
    "        # Train and evaluate the model on the current fold\n",
    "        validation_loss, _ = train_and_evaluate_transformer((int(hidden_size), int(num_layers), int(num_heads), optimizer, loss_func, int(patience), lr, shared_weight_decay, task_weight_decay, dropout), train_idx, val_idx)\n",
    "\n",
    "        fold_losses.append(validation_loss)\n",
    "\n",
    "    # Calculate the average validation loss across all folds\n",
    "    avg_validation_loss = sum(fold_losses) / len(fold_losses)\n",
    "\n",
    "    print(f\"Params: {(hidden_size, num_layers, optimizer, loss_func.__name__, patience, lr)}, Avg Validation Loss: {avg_validation_loss}\")\n",
    "\n",
    "    return avg_validation_loss\n",
    "\n",
    "# Perform Bayesian Optimization for RNN with baseline objective\n",
    "best_transformer_params_avg_loss = run_bayesian_optimization(objective_avg_loss, samples, transformer_param_space_avg_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **4.3 Train Transformer with Best Parameters**\n",
    "\n",
    "#### Now we will train the Transformer with the best parameters we found in our grid search and saved in **best_transformer_params**\n",
    "#### After we will print out a series of important metrics and visualizations to verify our results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.data.all import *\n",
    "from fastai.learner import *\n",
    "from fastai.metrics import *\n",
    "\n",
    "# To run with best parameters from previous runs\n",
    "# First run full parameter search with 60 samples and 20 epochs each with 5 fold CV\n",
    "# Best Transformer Parameters: {'hidden_size': 480, 'num_layers': 3, 'num_heads': np.int64(4), 'optimizer': np.str_('SGD'), 'loss_func': <function multitask_loss_loss_proportional at 0x000002D41F8F3D80>, 'patience': 15, 'lr': 0.006, 'shared_weight_decay': 0.00985175238373771, 'task_weight_decay': 0.00933427292776293, 'dropout': 0.1}\n",
    "# Best Transformer Validation Loss: 0.12026419788599015\n",
    "hidden_size = 480\n",
    "num_layers = 3\n",
    "num_heads = 4\n",
    "optimizer = 'SGD'\n",
    "multitask_loss_func = multitask_loss_loss_proportional\n",
    "lr = 0.006\n",
    "patience = 15\n",
    "dropout = 0.1\n",
    "epochs = 100\n",
    "shared_weight_decay = 0.00985175238373771\n",
    "task_weight_decay = 0.00933427292776293\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# print(\"Best Transformer Parameters:\", best_transformer_params_avg_loss)\n",
    "\n",
    "# # Extract parameters\n",
    "# dropout = best_transformer_params_avg_loss['dropout']\n",
    "# hidden_size = int(best_transformer_params_avg_loss['hidden_size'])\n",
    "# lr = best_transformer_params_avg_loss['lr']\n",
    "# num_heads = int(best_transformer_params_avg_loss['num_heads'])\n",
    "# num_layers = int(best_transformer_params_avg_loss['num_layers'])\n",
    "# multitask_loss_func = best_transformer_params_avg_loss['loss_func']\n",
    "# optimizer = best_transformer_params_avg_loss['optimizer']\n",
    "# patience = int(best_transformer_params_avg_loss['patience'])\n",
    "# shared_weight_decay = best_transformer_params_avg_loss['shared_weight_decay']\n",
    "# task_weight_decay = best_transformer_params_avg_loss['task_weight_decay']\n",
    "\n",
    "# Run the best Transformer model training with average validation loss objective parameters\n",
    "best_transformer_learn_avg_loss = run_best_transformer_model_training(hidden_size, num_layers, num_heads, optimizer, multitask_loss_func, patience, lr, shared_weight_decay, task_weight_decay, dropout, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **4.4 Visualize Our Results**\n",
    "\n",
    "#### Let us visualize the results of our model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate and visualize the model performance and results\n",
    "evaluate_and_visualize(learn=best_transformer_learn_avg_loss, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **4.5 Transformer Grid Search With Combined Score Objective**\n",
    "\n",
    "#### Let's do another grid search with a Combined Score Objective to see if we can find a better model. Here's how I've set up my hyperparameter optimization using Bayesian Optimization:\n",
    "\n",
    "#### This code implements **Bayesian Optimization** to tune hyperparameters for a Transformer model by optimizing a **combined objective function** that balances **F1 score** (to maximize) and **validation loss** (to minimize).\n",
    "\n",
    "### **Hyperparameter Search Space**\n",
    "\n",
    "The search space (`transformer_param_space`) includes:\n",
    "- **`hidden_size`**: The size of the Transformer hidden layer (integer range: 256–1024).\n",
    "- **`num_layers`**: The number of Transformer layers (integer range: 2–8).\n",
    "- **`num_heads`**: The number of attention heads (integer range: 2–16).\n",
    "- **`optimizer`**: A categorical choice between `'SGD'`, `'Adam'`, and `'AdamW'`.\n",
    "- **`loss_func`**: A categorical choice between custom multi-task loss functions.\n",
    "- **`patience`**: The patience parameter for early stopping (integer range: 3–15).\n",
    "- **`lr`**: The learning rate (real-valued range: 0.00001–0.006).\n",
    "- **`shared_weight_decay`**: Weight decay for shared layers (real-valued range: 0.0001–0.01).\n",
    "- **`task_weight_decay`**: Weight decay for task-specific layers (real-valued range: 0.0001–0.01).\n",
    "- **`dropout`**: Dropout rate (real-valued range: 0.1–0.5).\n",
    "\n",
    "### **Objective Function**\n",
    "\n",
    "The `objective_combined` function:\n",
    "- Performs **k-fold cross-validation** to evaluate the model's performance for each hyperparameter configuration.\n",
    "- Calls `train_and_evaluate_transformer` for each fold to train and evaluate the Transformer model.\n",
    "- Computes the **average F1 score** (to maximize) and **average validation loss** (to minimize) across all folds.\n",
    "- Combines these metrics into a single objective using weighted contributions:\n",
    "  - **F1 score**: Weighted higher (e.g., `0.7`) to prioritize maximizing performance.\n",
    "  - **Validation loss**: Weighted lower (e.g., `0.3`) to ensure regularization.\n",
    "\n",
    "### **Bayesian Optimization Execution**\n",
    "\n",
    "- The `run_bayesian_optimization` function minimizes the `objective_combined` function over the defined search space.\n",
    "- The best hyperparameters (`best_transformer_params`) are extracted after the optimization process.\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "This section optimizes the Transformer model's hyperparameters by balancing F1 score and validation loss using a combined objective. The use of Bayesian Optimization ensures efficient exploration of the hyperparameter space, while k-fold cross-validation provides robust evaluation. The result is a set of hyperparameters that maximizes F1 score while maintaining low validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter search space for Bayesian Optimization\n",
    "transformer_param_space_combined_score = [\n",
    "    Integer(name='hidden_size', low=256, high=1024),\n",
    "    Integer(name='num_layers', low=2, high=8),\n",
    "    Categorical(name='num_heads', categories=valid_num_heads),  # Use only valid num_heads\n",
    "    Categorical(name='optimizer', categories=['SGD', 'Adam', 'AdamW']),\n",
    "    Categorical(name='loss_func', categories=[multitask_loss, multitask_loss_loss_proportional, multitask_loss_auto_weighted_bce, multitask_loss_dynamic_weighted]),\n",
    "    Integer(name='patience', low=5, high=15),\n",
    "    Real_skopt(name='lr', low=0.00001, high=0.006),\n",
    "    Real_skopt(name='shared_weight_decay', low=0.0001, high=0.01),\n",
    "    Real_skopt(name='task_weight_decay', low=0.0001, high=0.01),\n",
    "    Real_skopt(name='dropout', low=0.1, high=0.5)\n",
    "]\n",
    "\n",
    "# Objective function for Bayesian Optimization with combined objective (f1 score and validation loss)\n",
    "@use_named_args(transformer_param_space_combined_score)\n",
    "def objective_combined(hidden_size, num_layers, num_heads, optimizer, loss_func, patience, lr, shared_weight_decay, task_weight_decay, dropout):\n",
    "    fold_f1_scores = []\n",
    "    fold_validation_losses = []\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    for train_idx, val_idx in kf.split(X_train):\n",
    "        validation_loss, f1_score = train_and_evaluate_transformer((int(hidden_size), int(num_layers), int(num_heads), optimizer, loss_func, int(patience), lr, shared_weight_decay, task_weight_decay, dropout), train_idx, val_idx)\n",
    "        fold_f1_scores.append(f1_score)\n",
    "        fold_validation_losses.append(validation_loss)\n",
    "\n",
    "    # Calculate the average F1 score and validation loss across all folds\n",
    "    avg_f1_score = sum(fold_f1_scores) / len(fold_f1_scores)\n",
    "    avg_validation_loss = sum(fold_validation_losses) / len(fold_validation_losses)\n",
    "\n",
    "    # Combine metrics into a single objective\n",
    "    weight_f1 = 0.8  # Adjust weights as needed\n",
    "    weight_loss = 0.2\n",
    "    combined_objective = -weight_f1 * avg_f1_score + weight_loss * avg_validation_loss\n",
    "\n",
    "    print(f\"Params: {(hidden_size, num_layers, num_heads, optimizer, loss_func.__name__, patience, lr)}\\nAvg F1 Score: {avg_f1_score} | Avg Validation Loss: {avg_validation_loss} | Combined Objective: {combined_objective}\")\n",
    "\n",
    "    return combined_objective\n",
    "\n",
    "# Perform Bayesian Optimization for RNN with baseline objective\n",
    "best_transformer_params_combined_score = run_bayesian_optimization(objective_combined, samples, transformer_param_space_combined_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **4.6 Train Transformer with Best Parameters for Combined Score**\n",
    "\n",
    "#### Now we will train the Transformer with the best parameters for our Combined Score we found in our grid search and saved in **best_transformer_params**\n",
    "#### After we will print out a series of important metrics and visualizations to verify our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.data.all import *\n",
    "from fastai.learner import *\n",
    "from fastai.metrics import *\n",
    "\n",
    "# To run with best parameters found during initial hyperparameter search with bayesian optimization and 60 samples and 20 epochs\n",
    "# Best Transformer Parameters: {'hidden_size': 1024, 'num_layers': 7, 'num_heads': np.int64(4), 'optimizer': np.str_('Adam'), 'loss_func': <function multitask_loss_loss_proportional at 0x000002419F837CE0>, 'patience': 5, 'lr': 1e-05, 'shared_weight_decay': 0.0001, 'task_weight_decay': 0.0001, 'dropout': 0.1}\n",
    "# Best Transformer Validation Loss: -0.482519742495292\n",
    "hidden_size = 1024\n",
    "num_layers = 7\n",
    "num_heads = 4\n",
    "optimizer = 'Adam'\n",
    "multitask_loss_func = multitask_loss_loss_proportional\n",
    "lr = 0.00001\n",
    "patience = 5\n",
    "dropout = 0.1\n",
    "shared_weight_decay = 0.0001\n",
    "task_weight_decay = 0.0001\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "\n",
    "# print(\"Best Transformer Parameters:\", best_transformer_params_combined_score)\n",
    "\n",
    "# # Extract parameters\n",
    "# dropout = best_transformer_params_combined_score['dropout']\n",
    "# hidden_size = int(best_transformer_params_combined_score['hidden_size'])\n",
    "# lr = best_transformer_params_combined_score['lr']\n",
    "# num_heads = int(best_transformer_params_combined_score['num_heads'])\n",
    "# num_layers = int(best_transformer_params_combined_score['num_layers'])\n",
    "# multitask_loss_func = best_transformer_params_combined_score['loss_func']\n",
    "# optimizer = best_transformer_params_combined_score['optimizer']\n",
    "# patience = int(best_transformer_params_combined_score['patience'])\n",
    "# shared_weight_decay = best_transformer_params_combined_score['shared_weight_decay']\n",
    "# task_weight_decay = best_transformer_params_combined_score['task_weight_decay']\n",
    "\n",
    "# Run the best Transformer model training with the combined score objective parameters\n",
    "best_transformer_learn_combined_score = run_best_transformer_model_training(hidden_size, num_layers, num_heads, optimizer, multitask_loss_func, patience, lr, shared_weight_decay, task_weight_decay, dropout, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **4.7 Visualize Our Results for Combined Score**\n",
    "#### Let us visualize the results of our model training and evaluation for our Combined Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate and visualize the model performance and results\n",
    "evaluate_and_visualize(learn=best_transformer_learn_combined_score, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Summary of the Project**\n",
    "**1. Data Pre-Processing**\n",
    "  - We followed the authors pre-processing code and used their serialized pipelines with limited refactoring.\n",
    "\n",
    "**2. Model Definition**\n",
    "  - We defined our `MultiTaskRNN` model for use with our experiments.\n",
    "  - We defined our 'MultiTaskTransformer1 model to use with our experiments.\n",
    "  - We defined a series of custom classes and helper methods including: `MultiTaskMetric`, `MultTaskTotalMetric`, `MultiTaskDataset`, multiple implementations of `multitask_loss`, `evaluate_and_visualize` to provide us a series of important metrics and visualizations, along with many other helper methods for convience in later tasks.\n",
    "\n",
    "**3. RNN Implementation**:\n",
    "  - Uses `nn.LSTM` for shared layers. Uses two task specifc branches with specifications defined by the authors.\n",
    "  - Performs Hyperameter Search with Bayesian Optimization over RNN-specific & optimizer-specific hyper parameters.\n",
    "  - We implement two seperate Objectives for Bayesian Optimization:\n",
    "    - One treatment that focuses on average validation loss\n",
    "    - One treatment that focuses on a combined weighted score of f1 score (which we are trying to maximize) and validation loss (which we are trying to minimize), giving more importance to f1 score.\n",
    "  - After completiong our Hyperparamter Search with Bayesian Optimization over each Objective we train and validate the model on the best set of parameters for each Objective.\n",
    "  - Finally we visualize our results across a series of valuable statistics like Correct vs Incorrect, F1 Score, Accuracy, Percesion@K, R Percision, and MAP.\n",
    "\n",
    "**4. Transformer Implementation**:\n",
    "  - Uses `nn.TransformerEncoder` for shared layers.\n",
    "  - Performs Hyperameter Search with Bayesian Optimization over transformer-specific & optimizer-specific hyper parameters.\n",
    "  - We implement two seperate Objectives for Bayesian Optimization:\n",
    "    - One treatment that focuses on average validation loss\n",
    "    - One treatment that focuses on a combined weighted score of f1 score (which we are trying to maximize) and validation loss (which we are trying to minimize), giving more importance to f1 score.\n",
    "  - After completiong our Hyperparamter Search with Bayesian Optimization over each Objective we train and validate the model on the best set of parameters for each Objective.\n",
    "  - Finally we visualize our results across a series of valuable statistics like Correct vs Incorrect, F1 Score, Accuracy, Percesion@K, R Percision, and MAP.\n",
    "\n",
    "#### In conclusion, both implementations are different takes on the work of **Gencheva et al (2019)** in the paper \"**A Context-Aware Approach for Detecting Worth-Checking Claims in Political Debates**\". And overall, this presented for a wonderful thought experiment around the intersectional work between political science and artificial intelligence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CourseProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
